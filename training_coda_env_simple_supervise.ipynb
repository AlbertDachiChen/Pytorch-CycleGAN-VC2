{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import time\n",
    "import librosa\n",
    "import pickle\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "import preprocess\n",
    "from trainingDataset import trainingDataset, trainingDataset_Paired\n",
    "from model_VC2 import Generator, Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "class GLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GLU, self).__init__()\n",
    "        # Custom Implementation because the Voice Conversion Cycle GAN\n",
    "        # paper assumes GLU won't reduce the dimension of tensor by 2.\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input * torch.sigmoid(input)\n",
    "\n",
    "\n",
    "class up_2Dsample(nn.Module):\n",
    "    def __init__(self, upscale_factor=2):\n",
    "        super(up_2Dsample, self).__init__()\n",
    "        self.scale_factor = upscale_factor\n",
    "\n",
    "    def forward(self, input):\n",
    "        h = input.shape[2]\n",
    "        w = input.shape[3]\n",
    "        new_size = [h * self.scale_factor, w * self.scale_factor]\n",
    "        return F.interpolate(input,new_size)\n",
    "       \n",
    "\n",
    "class PixelShuffle(nn.Module):\n",
    "    def __init__(self, upscale_factor=2):\n",
    "        super(PixelShuffle, self).__init__()\n",
    "        # Custom Implementation because PyTorch PixelShuffle requires,\n",
    "        # 4D input. Whereas, in this case we have have 3D array\n",
    "        self.upscale_factor = upscale_factor\n",
    "\n",
    "    def forward(self, input):\n",
    "        n = input.shape[0]\n",
    "        c_out = input.shape[1] // self.upscale_factor\n",
    "        w_new = input.shape[2] * self.upscale_factor\n",
    "        return input.view(n, c_out, w_new)\n",
    "\n",
    "\n",
    "class ResidualLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super(ResidualLayer, self).__init__()\n",
    "        self.conv1d_layer = nn.Sequential(nn.Conv1d(in_channels=in_channels,\n",
    "                                                    out_channels=out_channels,\n",
    "                                                    kernel_size=kernel_size,\n",
    "                                                    stride=1,\n",
    "                                                    padding=padding),\n",
    "                                          nn.InstanceNorm1d(num_features=out_channels,\n",
    "                                                            affine=True))\n",
    "\n",
    "        self.conv_layer_gates = nn.Sequential(nn.Conv1d(in_channels=in_channels,\n",
    "                                                        out_channels=out_channels,\n",
    "                                                        kernel_size=kernel_size,\n",
    "                                                        stride=1,\n",
    "                                                        padding=padding),\n",
    "                                              nn.InstanceNorm1d(num_features=out_channels,\n",
    "                                                                affine=True))\n",
    "\n",
    "        self.conv1d_out_layer = nn.Sequential(nn.Conv1d(in_channels=out_channels,\n",
    "                                                        out_channels=in_channels,\n",
    "                                                        kernel_size=kernel_size,\n",
    "                                                        stride=1,\n",
    "                                                        padding=padding),\n",
    "                                              nn.InstanceNorm1d(num_features=in_channels,\n",
    "                                                                affine=True))\n",
    "\n",
    "    def forward(self, input):\n",
    "        #print(\"input size: \", input.size())\n",
    "        h1_norm = self.conv1d_layer(input)\n",
    "        h1_gates_norm = self.conv_layer_gates(input)\n",
    "\n",
    "        # GLU\n",
    "        h1_glu = h1_norm * torch.sigmoid(h1_gates_norm)\n",
    "\n",
    "        h2_norm = self.conv1d_out_layer(h1_glu)\n",
    "        return input + h2_norm # (B, C, T)\n",
    "\n",
    "\n",
    "class downSample_Generator(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super(downSample_Generator, self).__init__()\n",
    "\n",
    "        self.convLayer = nn.Sequential(nn.Conv2d(in_channels=in_channels,\n",
    "                                                 out_channels=out_channels,\n",
    "                                                 kernel_size=kernel_size,\n",
    "                                                 stride=stride,\n",
    "                                                 padding=padding),\n",
    "                                       nn.InstanceNorm2d(num_features=out_channels,\n",
    "                                                         affine=True))\n",
    "        self.convLayer_gates = nn.Sequential(nn.Conv2d(in_channels=in_channels,\n",
    "                                                       out_channels=out_channels,\n",
    "                                                       kernel_size=kernel_size,\n",
    "                                                       stride=stride,\n",
    "                                                       padding=padding),\n",
    "                                             nn.InstanceNorm2d(num_features=out_channels,\n",
    "                                                               affine=True))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.convLayer(input) * torch.sigmoid(self.convLayer_gates(input))\n",
    "\n",
    "\n",
    "class upSample_Generator(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super(upSample_Generator, self).__init__()\n",
    "\n",
    "        self.convLayer = nn.Sequential(nn.Conv2d(in_channels=in_channels,\n",
    "                                                 out_channels=out_channels,\n",
    "                                                 kernel_size=kernel_size,\n",
    "                                                 stride=stride,\n",
    "                                                 padding=padding),\n",
    "                                       #PixelShuffle(upscale_factor=2),\n",
    "                                       up_2Dsample(upscale_factor=2),\n",
    "                                       nn.InstanceNorm2d(num_features=out_channels,\n",
    "                                                         affine=True))\n",
    "        self.convLayer_gates = nn.Sequential(nn.Conv2d(in_channels=in_channels,\n",
    "                                                       out_channels=out_channels,\n",
    "                                                       kernel_size=kernel_size,\n",
    "                                                       stride=stride,\n",
    "                                                       padding=padding),\n",
    "                                             #PixelShuffle(upscale_factor=2),\n",
    "                                             up_2Dsample(upscale_factor=2),\n",
    "                                             nn.InstanceNorm2d(num_features=out_channels,\n",
    "                                                               affine=True))\n",
    "    def forward(self, input):        \n",
    "        return self.convLayer(input) * torch.sigmoid(self.convLayer_gates(input))\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1,\n",
    "                               out_channels=128,\n",
    "                               kernel_size=[5,15],\n",
    "                               stride=1,\n",
    "                               padding=[2,7])\n",
    "\n",
    "        self.conv1_gates = nn.Conv2d(in_channels=1,\n",
    "                               out_channels=128,\n",
    "                               kernel_size=[5,15],\n",
    "                               stride=1,\n",
    "                               padding=[2,7])\n",
    "\n",
    "        # Downsample Layer\n",
    "        self.downSample1 = downSample_Generator(in_channels=128,\n",
    "                                                out_channels=256,\n",
    "                                                kernel_size=5,\n",
    "                                                stride=2,\n",
    "                                                padding=2)\n",
    "\n",
    "        self.downSample2 = downSample_Generator(in_channels=256,\n",
    "                                                out_channels=512,\n",
    "                                                kernel_size=5,\n",
    "                                                stride=2,\n",
    "                                                padding=2)\n",
    "        #reshape\n",
    "        self.conv2 = nn.Conv1d(in_channels=3072,\n",
    "                               out_channels=512,\n",
    "                               kernel_size=1,\n",
    "                               stride=1)\n",
    "\n",
    "        # Residual Blocks\n",
    "        self.residualLayer1 = ResidualLayer(in_channels=512,\n",
    "                                            out_channels=1024,\n",
    "                                            kernel_size=3,\n",
    "                                            stride=1,\n",
    "                                            padding=1)\n",
    "        self.residualLayer2 = ResidualLayer(in_channels=512,\n",
    "                                            out_channels=1024,\n",
    "                                            kernel_size=3,\n",
    "                                            stride=1,\n",
    "                                            padding=1)\n",
    "        self.residualLayer3 = ResidualLayer(in_channels=512,\n",
    "                                            out_channels=1024,\n",
    "                                            kernel_size=3,\n",
    "                                            stride=1,\n",
    "                                            padding=1)\n",
    "    def forward(self, input):\n",
    "        # GLU\n",
    "        input = input.unsqueeze(1)\n",
    "        conv1 = self.conv1(input) * torch.sigmoid(self.conv1_gates(input))\n",
    "        # print(\"shape of conv1, \", conv1.size())\n",
    "        downsample1 = self.downSample1(conv1)\n",
    "        # print(\"shape of downsample1, \", downsample1.size())\n",
    "        self.downsample2_forshape = self.downSample2(downsample1)\n",
    "        downsample3 = self.downsample2_forshape.view([self.downsample2_forshape.shape[0],-1,self.downsample2_forshape.shape[3]])\n",
    "        downsample3 = self.conv2(downsample3)\n",
    "        # print(\"shape of downsample3, \", downsample3.size())\n",
    "        residual_layer_1 = self.residualLayer1(downsample3)\n",
    "        residual_layer_2 = self.residualLayer2(residual_layer_1)\n",
    "        # print(\"shape of residual_layer_2, \", residual_layer_2.size())\n",
    "        residual_layer_3 = self.residualLayer3(residual_layer_2)\n",
    "        # print(\"shape of residual_layer_3, \", residual_layer_3.size())\n",
    "        return residual_layer_3\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.residualLayer4 = ResidualLayer(in_channels=512,\n",
    "                                            out_channels=1024,\n",
    "                                            kernel_size=3,\n",
    "                                            stride=1,\n",
    "                                            padding=1)\n",
    "        self.residualLayer5 = ResidualLayer(in_channels=512,\n",
    "                                            out_channels=1024,\n",
    "                                            kernel_size=3,\n",
    "                                            stride=1,\n",
    "                                            padding=1)\n",
    "        self.residualLayer6 = ResidualLayer(in_channels=512,\n",
    "                                            out_channels=1024,\n",
    "                                            kernel_size=3,\n",
    "                                            stride=1,\n",
    "                                            padding=1)\n",
    "        #reshape\n",
    "        self.conv3 = nn.Conv1d(in_channels=512,\n",
    "                               out_channels=3072,\n",
    "                               kernel_size=1,\n",
    "                               stride=1)\n",
    "\n",
    "\n",
    "        # UpSample Layer\n",
    "        self.upSample1 = upSample_Generator(in_channels=512,\n",
    "                                            out_channels=1024,\n",
    "                                            kernel_size=5,\n",
    "                                            stride=1,\n",
    "                                            padding=2)\n",
    "        \n",
    "        self.upSample2 = upSample_Generator(in_channels=1024,\n",
    "                                            out_channels=512,\n",
    "                                            kernel_size=5,\n",
    "                                            stride=1,\n",
    "                                            padding=2)\n",
    "\n",
    "        self.lastConvLayer = nn.Conv2d(in_channels=512,\n",
    "                                       out_channels=1,\n",
    "                                       kernel_size=[5,15],\n",
    "                                       stride=1,\n",
    "                                       padding=[2,7])\n",
    "\n",
    "    def forward(self, input, shapes):\n",
    "        # GLU\n",
    "        residual_layer_4 = self.residualLayer4(input)\n",
    "        residual_layer_5 = self.residualLayer5(residual_layer_4)\n",
    "        residual_layer_6 = self.residualLayer6(residual_layer_5)\n",
    "        residual_layer_6 = self.conv3(residual_layer_6)\n",
    "        residual_layer_6 = residual_layer_6.view([shapes[0],shapes[1],shapes[2],shapes[3]])\n",
    "        \n",
    "        upSample_layer_1 = self.upSample1(residual_layer_6)\n",
    "        upSample_layer_2 = self.upSample2(upSample_layer_1)\n",
    "        output = self.lastConvLayer(upSample_layer_2)\n",
    "        output = output.view([output.shape[0],-1,output.shape[3]])\n",
    "        return output\n",
    "\n",
    "\n",
    "class CNN_Discriminator(nn.Module):\n",
    "    def __init__(self, Cin=1024, Tmax=256): # all inputs are padded to Tmax \n",
    "        super(CNN_Discriminator, self).__init__()\n",
    "        self.Tmax = Tmax\n",
    "        Conv_dim = 256\n",
    "        self.conv = nn.Conv1d(in_channels=Cin,\n",
    "                              out_channels=Conv_dim,\n",
    "                              kernel_size=3,\n",
    "                              stride=1)\n",
    "        self.avgPool = nn.AvgPool1d(kernel_size=Tmax-2)\n",
    "        self.linear = nn.Linear(Conv_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x): # input tensor in shape (B, Cin, Tin)\n",
    "        # print(\"input size at discriminator ,\", x.size())\n",
    "        B, Cin, Tin = x.shape\n",
    "        x = x.unsqueeze(1) # (B, 1, Cin, Tin)\n",
    "        # print(\"input size after unsqueeze ,\", x.size())\n",
    "        Tpad_right = self.Tmax - Tin\n",
    "        padder = nn.ZeroPad2d((0,Tpad_right,0,0))\n",
    "        x = padder(x) # (B, 1, Cin, Tmax)\n",
    "        # print(\"input size after pad ,\", x.size())\n",
    "        x = x.squeeze() # (B, Cin, Tmax)\n",
    "        # print(\"input size before conv ,\", x.size())\n",
    "        x = self.conv(x) # (B, Conv_dim, Tmax-2)\n",
    "        x = self.avgPool(x) # (B, Conv_dim, 1)\n",
    "        x = x.squeeze() # (B, Conv_dim)\n",
    "        x = self.linear(x) # (B, 1)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "class RNN_Discriminator(nn.Module):\n",
    "    def __init__(self, Cin=1024): # all inputs are padded to Tmax \n",
    "        super(RNN_Discriminator, self).__init__()\n",
    "        Conv_dim = 256\n",
    "        self.conv = nn.Conv1d(in_channels=Cin,\n",
    "                              out_channels=Conv_dim,\n",
    "                              kernel_size=3,\n",
    "                              stride=1)\n",
    "        hidden = 256\n",
    "        self.gru = nn.GRU(input_size=Conv_dim, \n",
    "                          hidden_size=hidden)\n",
    "        self.linear = nn.Linear(hidden, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x): # input tensor in shape (B, Cin, Tin)\n",
    "        B, Cin, Tin = x.shape\n",
    "        x = self.conv(x) # (B, Conv_dim, Tin-2)\n",
    "        x = x.permute(2, 0, 1) # (Tin-2, B, Conv_dim)\n",
    "        x, hout = self.gru(x) # hout: (1, B, hidden)\n",
    "        hout = hout.squeeze() # hout: (B, hidden)\n",
    "        hout = self.linear(hout) # (B, 1)\n",
    "        hout = self.sigmoid(hout) # (B, 1)\n",
    "        return hout\n",
    "\n",
    "\n",
    "\n",
    "class DownSample_Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super(DownSample_Discriminator, self).__init__()\n",
    "\n",
    "        self.convLayer = nn.Sequential(nn.Conv2d(in_channels=in_channels,\n",
    "                                                 out_channels=out_channels,\n",
    "                                                 kernel_size=kernel_size,\n",
    "                                                 stride=stride,\n",
    "                                                 padding=padding),\n",
    "                                       nn.InstanceNorm2d(num_features=out_channels,\n",
    "                                                         affine=True))\n",
    "        self.convLayerGates = nn.Sequential(nn.Conv2d(in_channels=in_channels,\n",
    "                                                      out_channels=out_channels,\n",
    "                                                      kernel_size=kernel_size,\n",
    "                                                      stride=stride,\n",
    "                                                      padding=padding),\n",
    "                                            nn.InstanceNorm2d(num_features=out_channels,\n",
    "                                                              affine=True))\n",
    "\n",
    "    def forward(self, input):\n",
    "        # GLU\n",
    "        return self.convLayer(input) * torch.sigmoid(self.convLayerGates(input))\n",
    "        \n",
    "\n",
    "class RealFake_Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RealFake_Discriminator, self).__init__()\n",
    "\n",
    "        self.convLayer1 = nn.Conv2d(in_channels=1,\n",
    "                                    out_channels=128,\n",
    "                                    kernel_size=[3, 3],\n",
    "                                    stride=[1, 1])\n",
    "        self.convLayer1_gates = nn.Conv2d(in_channels=1,\n",
    "                                          out_channels=128,\n",
    "                                          kernel_size=[3, 3],\n",
    "                                          stride=[1, 1])\n",
    "\n",
    "        # Note: Kernel Size have been modified in the PyTorch implementation\n",
    "        # compared to the actual paper, as to retain dimensionality. Unlike,\n",
    "        # TensorFlow, PyTorch doesn't have padding='same', hence, kernel sizes\n",
    "        # were altered to retain the dimensionality after each layer\n",
    "\n",
    "        # DownSample Layer\n",
    "        self.downSample1 = DownSample_Discriminator(in_channels=128,\n",
    "                                                    out_channels=256,\n",
    "                                                    kernel_size=[3, 3],\n",
    "                                                    stride=[2, 2],\n",
    "                                                    padding=0)\n",
    "\n",
    "        self.downSample2 = DownSample_Discriminator(in_channels=256,\n",
    "                                                    out_channels=512,\n",
    "                                                    kernel_size=[3, 3],\n",
    "                                                    stride=[2, 2],\n",
    "                                                    padding=0)\n",
    "\n",
    "        self.downSample3 = DownSample_Discriminator(in_channels=512,\n",
    "                                                    out_channels=1024,\n",
    "                                                    kernel_size=[3, 3],\n",
    "                                                    stride=[2, 2],\n",
    "                                                    padding=0)\n",
    "\n",
    "        self.downSample4 = DownSample_Discriminator(in_channels=1024,\n",
    "                                                    out_channels=1024,\n",
    "                                                    kernel_size=[1, 5],\n",
    "                                                    stride=[1, 1],\n",
    "                                                    padding=[0, 2])\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        self.fc = nn.Linear(in_features=1024,\n",
    "                            out_features=1)\n",
    "\n",
    "        # output Layer\n",
    "        self.output_layer = nn.Conv2d(in_channels=1024,\n",
    "                                      out_channels=1,\n",
    "                                      kernel_size=[1, 3],\n",
    "                                      stride=[1, 1],\n",
    "                                      padding=[0, 1])\n",
    "\n",
    "    def forward(self, input):\n",
    "        # input has shape [batch_size, num_features, time]\n",
    "        # discriminator requires shape [batchSize, 1, num_features, time]\n",
    "        input = input.unsqueeze(1)\n",
    "        # GLU\n",
    "        pad_input = nn.ZeroPad2d((1, 1, 1, 1))\n",
    "        layer1 = self.convLayer1(\n",
    "            pad_input(input)) * torch.sigmoid(self.convLayer1_gates(pad_input(input)))\n",
    "\n",
    "        pad_input = nn.ZeroPad2d((1, 0, 1, 0))\n",
    "        downSample1 = self.downSample1(pad_input(layer1))\n",
    "\n",
    "        pad_input = nn.ZeroPad2d((1, 0, 1, 0))\n",
    "        downSample2 = self.downSample2(pad_input(downSample1))\n",
    "\n",
    "        pad_input = nn.ZeroPad2d((1, 0, 1, 0))\n",
    "        downSample3 = self.downSample3(pad_input(downSample2))\n",
    "\n",
    "        downSample4 = self.downSample4(downSample3)\n",
    "        downSample4 = self.output_layer(downSample4)\n",
    "\n",
    "        downSample4 = downSample4.contiguous().permute(0, 2, 3, 1).contiguous()\n",
    "        # fc = torch.sigmoid(self.fc(downSample3))\n",
    "        # Taking off sigmoid layer to avoid vanishing gradient problem\n",
    "        #fc = self.fc(downSample4)\n",
    "        fc = torch.sigmoid(downSample4)\n",
    "        return fc\n",
    "\n",
    "class AB_Discriminator_RNN(nn.Module):\n",
    "    def __init__(self, Cin=512): # (B, Cin, Tin)\n",
    "        super(AB_Discriminator_RNN, self).__init__()\n",
    "\n",
    "        self.convLayer1 = nn.Conv2d(in_channels=1,\n",
    "                                    out_channels=128,\n",
    "                                    kernel_size=[3, 3],\n",
    "                                    stride=[1, 1])\n",
    "        self.convLayer1_gates = nn.Conv2d(in_channels=1,\n",
    "                                          out_channels=128,\n",
    "                                          kernel_size=[3, 3],\n",
    "                                          stride=[1, 1])\n",
    "\n",
    "        # Note: Kernel Size have been modified in the PyTorch implementation\n",
    "        # compared to the actual paper, as to retain dimensionality. Unlike,\n",
    "        # TensorFlow, PyTorch doesn't have padding='same', hence, kernel sizes\n",
    "        # were altered to retain the dimensionality after each layer\n",
    "\n",
    "        # DownSample Layer\n",
    "        self.downSample1 = DownSample_Discriminator(in_channels=128,\n",
    "                                                    out_channels=256,\n",
    "                                                    kernel_size=[3, 3],\n",
    "                                                    stride=[2, 2],\n",
    "                                                    padding=0)\n",
    "\n",
    "        self.downSample2 = DownSample_Discriminator(in_channels=256,\n",
    "                                                    out_channels=512,\n",
    "                                                    kernel_size=[3, 3],\n",
    "                                                    stride=[2, 2],\n",
    "                                                    padding=0)\n",
    "\n",
    "        self.downSample3 = DownSample_Discriminator(in_channels=512,\n",
    "                                                    out_channels=1024,\n",
    "                                                    kernel_size=[3, 3],\n",
    "                                                    stride=[2, 2],\n",
    "                                                    padding=0)\n",
    "\n",
    "        self.downSample4 = DownSample_Discriminator(in_channels=1024,\n",
    "                                                    out_channels=1024,\n",
    "                                                    kernel_size=[1, 5],\n",
    "                                                    stride=[1, 1],\n",
    "                                                    padding=[0, 2])\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        self.fc = nn.Linear(in_features=1024,\n",
    "                            out_features=1)\n",
    "\n",
    "        # output Layer\n",
    "        self.output_layer = nn.Conv2d(in_channels=1024,\n",
    "                                      out_channels=1,\n",
    "                                      kernel_size=[1, 3],\n",
    "                                      stride=[1, 1],\n",
    "                                      padding=[0, 1])\n",
    "\n",
    "    def forward(self, input):\n",
    "        # input has shape [batch_size, num_features, time]\n",
    "        # discriminator requires shape [batchSize, 1, num_features, time]\n",
    "        input = input.unsqueeze(1)\n",
    "        # GLU\n",
    "        pad_input = nn.ZeroPad2d((1, 1, 1, 1))\n",
    "        layer1 = self.convLayer1(\n",
    "            pad_input(input)) * torch.sigmoid(self.convLayer1_gates(pad_input(input)))\n",
    "\n",
    "        pad_input = nn.ZeroPad2d((1, 0, 1, 0))\n",
    "        downSample1 = self.downSample1(pad_input(layer1))\n",
    "\n",
    "        pad_input = nn.ZeroPad2d((1, 0, 1, 0))\n",
    "        downSample2 = self.downSample2(pad_input(downSample1))\n",
    "\n",
    "        pad_input = nn.ZeroPad2d((1, 0, 1, 0))\n",
    "        downSample3 = self.downSample3(pad_input(downSample2))\n",
    "\n",
    "        downSample4 = self.downSample4(downSample3)\n",
    "        downSample4 = self.output_layer(downSample4)\n",
    "\n",
    "        downSample4 = downSample4.contiguous().permute(0, 2, 3, 1).contiguous()\n",
    "        # fc = torch.sigmoid(self.fc(downSample3))\n",
    "        # Taking off sigmoid layer to avoid vanishing gradient problem\n",
    "        #fc = self.fc(downSample4)\n",
    "        fc = torch.sigmoid(downSample4)\n",
    "        return fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def adjust_lr_rate(optimizer, name='generator'):\n",
    "    global generator_lr, generator_lr_decay, discriminator_lr, discriminator_lr_decay\n",
    "    if name == 'generator':\n",
    "        generator_lr = max(\n",
    "            0., generator_lr - generator_lr_decay)\n",
    "        for param_groups in optimizer.param_groups:\n",
    "            param_groups['lr'] = generator_lr\n",
    "    else:\n",
    "        discriminator_lr = max(\n",
    "            0., discriminator_lr - discriminator_lr_decay)\n",
    "        for param_groups in optimizer.param_groups:\n",
    "            param_groups['lr'] = discriminator_lr\n",
    "\n",
    "def reset_grad():\n",
    "    encoder_noCNNRNN_optimizer.zero_grad()\n",
    "    decoder_2A_noCNNRNN_optimizer.zero_grad()\n",
    "    decoder_2B_noCNNRNN_optimizer.zero_grad()\n",
    "    \n",
    "    realfake_discriminator_B_optimizer.zero_grad()\n",
    "    realfake_discriminator_A_optimizer.zero_grad()\n",
    "#     rnn_discriminator_optimizer.zero_grad()\n",
    "\n",
    "def savePickle(variable, fileName):\n",
    "    with open(fileName, 'wb') as f:\n",
    "        pickle.dump(variable, f)\n",
    "\n",
    "def loadPickleFile(fileName):\n",
    "    with open(fileName, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def store_to_file(doc):\n",
    "    doc = doc + \"\\n\"\n",
    "    with open(file_name, \"a\") as myfile:\n",
    "        myfile.write(doc)\n",
    "\n",
    "def saveModelCheckPoint(epoch, PATH):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'generator_cycle_loss_store': generator_cycle_loss_store,\n",
    "        'generator_identity_loss_store': generator_identity_loss_store,\n",
    "        'generator_supervise_loss_store':generator_supervise_loss_store,\n",
    "        'RF_discriminator_loss_store': RF_discriminator_loss_store,\n",
    "        'embedding_loss_store': embedding_loss_store,\n",
    "        \n",
    "        'encoder_noCNNRNN_state_dict': encoder_noCNNRNN.state_dict(),\n",
    "        'decoder_2A_noCNNRNN_state_dict': decoder_2A_noCNNRNN.state_dict(),\n",
    "        'decoder_2B_noCNNRNN_state_dict': decoder_2B_noCNNRNN.state_dict(),\n",
    "#         'rnn_discriminator_state_dict': rnn_discriminator.state_dict(),\n",
    "        'realfake_discriminator_A_state_dict': realfake_discriminator_A.state_dict(),\n",
    "        'realfake_discriminator_B_state_dict': realfake_discriminator_B.state_dict(),\n",
    "        \n",
    "        'encoder_noCNNRNN_optimizer_state_dict': encoder_noCNNRNN_optimizer.state_dict(),\n",
    "        'decoder_2A_noCNNRNN_optimizer_state_dict': decoder_2A_noCNNRNN_optimizer.state_dict(),\n",
    "        'decoder_2B_noCNNRNN_optimizer_state_dict': decoder_2B_noCNNRNN_optimizer.state_dict(),\n",
    "        'realfake_discriminator_A_optimizer_state_dict': realfake_discriminator_A_optimizer.state_dict(),\n",
    "        'realfake_discriminator_B_optimizer_state_dict': realfake_discriminator_B_optimizer.state_dict(),\n",
    "#         'rnn_discriminator_optimizer_state_dict': rnn_discriminator_optimizer.state_dict(),\n",
    "        \n",
    "        \n",
    "    }, PATH)\n",
    "\n",
    "def loadModel(PATH):\n",
    "    checkPoint = torch.load(PATH)\n",
    "    epoch = int(checkPoint['epoch']) + 1\n",
    "    generator_cycle_loss_store = checkPoint['generator_cycle_loss_store']\n",
    "    generator_identity_loss_store = checkPoint['generator_identity_loss_store']\n",
    "    #generator_supervise_loss_store = checkPoint['generator_supervise_loss_store']\n",
    "    RF_discriminator_loss_store = checkPoint['RF_discriminator_loss_store']\n",
    "    embedding_loss_store = checkPoint['embedding_loss_store']\n",
    "    \n",
    "    encoder_noCNNRNN.load_state_dict(\n",
    "        state_dict=checkPoint['encoder_noCNNRNN_state_dict'])\n",
    "    decoder_2A_noCNNRNN.load_state_dict(\n",
    "        state_dict=checkPoint['decoder_2A_noCNNRNN_state_dict'])\n",
    "    decoder_2B_noCNNRNN.load_state_dict(\n",
    "        state_dict=checkPoint['decoder_2B_noCNNRNN_state_dict'])\n",
    "#     rnn_discriminator.load_state_dict(\n",
    "#         state_dict=checkPoint['rnn_discriminator_state_dict'])\n",
    "    realfake_discriminator_A.load_state_dict(\n",
    "        state_dict=checkPoint['realfake_discriminator_A_state_dict'])\n",
    "    realfake_discriminator_B.load_state_dict(\n",
    "        state_dict=checkPoint['realfake_discriminator_B_state_dict'])\n",
    "    \n",
    "    encoder_noCNNRNN_optimizer.load_state_dict(\n",
    "        state_dict=checkPoint['encoder_noCNNRNN_optimizer_state_dict'])\n",
    "    decoder_2A_noCNNRNN_optimizer.load_state_dict(\n",
    "        state_dict=checkPoint['decoder_2A_noCNNRNN_optimizer_state_dict'])\n",
    "    decoder_2B_noCNNRNN_optimizer.load_state_dict(\n",
    "        state_dict=checkPoint['decoder_2B_noCNNRNN_optimizer_state_dict'])\n",
    "    realfake_discriminator_A_optimizer.load_state_dict(\n",
    "        state_dict=checkPoint['realfake_discriminator_A_optimizer_state_dict'])\n",
    "    realfake_discriminator_B_optimizer.load_state_dict(\n",
    "        state_dict=checkPoint['realfake_discriminator_B_optimizer_state_dict'])\n",
    "#     rnn_discriminator_optimizer.load_state_dict(\n",
    "#         state_dict=checkPoint['rnn_discriminator_optimizer_state_dict'])\n",
    "    \n",
    "    return epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up\n",
    "logf0s_normalization = \"./cache/logf0s_normalization.npz\"\n",
    "mcep_normalization = \"./cache/mcep_normalization.npz\"\n",
    "coded_sps_A_norm = \"./cache/coded_sps_A_norm.pickle\"\n",
    "coded_sps_B_norm = \"./cache/coded_sps_B_norm.pickle\" \n",
    "resume_training_at = \"./cache/model_checkpoint/_CycleGAN_CheckPoint\"\n",
    "validation_A_dir = \"./data/evaluation_all/SF1/\" \n",
    "output_A_dir = \"./data/vcc2016_training/converted_sound/SF1\"\n",
    "validation_B_dir = \"./data/evaluation_all/TF2/\" \n",
    "output_B_dir = \"./data/vcc2016_training/converted_sound/TF2/\"\n",
    "# =================================================\n",
    "mini_batch_size = 1\n",
    "dataset_A = loadPickleFile(coded_sps_A_norm)\n",
    "dataset_B = loadPickleFile(coded_sps_B_norm)\n",
    "device = torch.device(\n",
    "    'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Speech Parameters\n",
    "logf0s_normalization = np.load(logf0s_normalization)\n",
    "log_f0s_mean_A = logf0s_normalization['mean_A']\n",
    "log_f0s_std_A = logf0s_normalization['std_A']\n",
    "log_f0s_mean_B = logf0s_normalization['mean_B']\n",
    "log_f0s_std_B = logf0s_normalization['std_B']\n",
    "\n",
    "mcep_normalization = np.load(mcep_normalization)\n",
    "coded_sps_A_mean = mcep_normalization['mean_A']\n",
    "coded_sps_A_std = mcep_normalization['std_A']\n",
    "coded_sps_B_mean = mcep_normalization['mean_B']\n",
    "coded_sps_B_std = mcep_normalization['std_B']\n",
    "\n",
    "# Encoder and Decoder\n",
    "encoder_noCNNRNN = Encoder().to(device)\n",
    "decoder_2A_noCNNRNN = Decoder().to(device)\n",
    "decoder_2B_noCNNRNN = Decoder().to(device)\n",
    "\n",
    "# Discriminator\n",
    "#rnn_discriminator = AB_Discriminator_RNN(Cin=512).to(device)\n",
    "realfake_discriminator_A = RealFake_Discriminator().to(device)\n",
    "realfake_discriminator_B = RealFake_Discriminator().to(device)\n",
    "\n",
    "# Loss Functions\n",
    "criterion_mse = torch.nn.MSELoss()\n",
    "\n",
    "# Initial learning rates\n",
    "generator_lr = 0.0002\n",
    "discriminator_lr = 0.0001\n",
    "\n",
    "# Learning rate decay\n",
    "generator_lr_decay = generator_lr / 200000\n",
    "discriminator_lr_decay = discriminator_lr / 200000\n",
    "\n",
    "# Optimizers\n",
    "def get_optimizer(one_module, learning_rate):\n",
    "    return torch.optim.Adam(list(one_module.parameters()), lr=learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "encoder_noCNNRNN_optimizer = get_optimizer(encoder_noCNNRNN, generator_lr)\n",
    "\n",
    "decoder_2A_noCNNRNN_optimizer = get_optimizer(decoder_2A_noCNNRNN, generator_lr)\n",
    "decoder_2B_noCNNRNN_optimizer = get_optimizer(decoder_2B_noCNNRNN, generator_lr)\n",
    "\n",
    "generators_optimizers = [\n",
    "    encoder_noCNNRNN_optimizer,\n",
    "    decoder_2A_noCNNRNN_optimizer,\n",
    "    decoder_2B_noCNNRNN_optimizer\n",
    "]\n",
    "realfake_discriminator_A_optimizer = get_optimizer(realfake_discriminator_A, discriminator_lr)\n",
    "realfake_discriminator_B_optimizer = get_optimizer(realfake_discriminator_B, discriminator_lr)\n",
    "# rnn_discriminator_optimizer = get_optimizer(rnn_discriminator, discriminator_lr)\n",
    "\n",
    "# Storing Discriminatior and Generator Loss\n",
    "generator_cycle_loss_store = []\n",
    "generator_identity_loss_store = []\n",
    "generator_supervise_loss_store = []\n",
    "RF_discriminator_loss_store = []\n",
    "embedding_loss_store = []\n",
    "\n",
    "start_epoch = 0\n",
    "\n",
    "# To Load save previously saved models\n",
    "# load_path = \"/home/frank/Documents/experiments/model3_supervise/0/checkpoint\"\n",
    "# start_epoch = loadModel(load_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generators_reset_grad():\n",
    "    for opt in generators_optimizers:\n",
    "        opt.zero_grad()\n",
    "\n",
    "def generators_update():\n",
    "    for opt in generators_optimizers:\n",
    "        opt.step()\n",
    "\n",
    "def encoders_forward(voice_noCNNRNN):\n",
    "    embedding_noCNNRNN = encoder_noCNNRNN(voice_noCNNRNN)\n",
    "    return embedding_noCNNRNN\n",
    "\n",
    "def decoders_forward(embedding_noCNNRNN, isToA=True):\n",
    "    decoder_noCNNRNN = decoder_2A_noCNNRNN if isToA else decoder_2B_noCNNRNN\n",
    "    fake_noCNNRNN = decoder_noCNNRNN(embedding_noCNNRNN, encoder_noCNNRNN.downsample2_forshape.shape)\n",
    "    return fake_noCNNRNN\n",
    "\n",
    "def AB_discriminator_zero_grad():\n",
    "    rnn_discriminator_optimizer.zero_grad()\n",
    "\n",
    "def AB_discriminator_update():\n",
    "    rnn_discriminator_optimizer.step()\n",
    "\n",
    "def AB_discriminator_forward_loss(emb_noCNNRNN, isA=True, gradients_for_discriminator=True):    \n",
    "    true_pred = 0.0 if isA else 1.0\n",
    "    target_rnn_00 = true_pred if gradients_for_discriminator else 0.5\n",
    "    \n",
    "    pred_noCNNRNN_rnn_discriminator = rnn_discriminator(emb_noCNNRNN)\n",
    "    \n",
    "    d_loss = torch.mean((target_rnn_00 - pred_noCNNRNN_rnn_discriminator) ** 2)\n",
    "    return d_loss\n",
    "\n",
    "def realfake_discriminator_zero_grad(isA=True):\n",
    "    if isA:\n",
    "        realfake_discriminator_A_optimizer.zero_grad()\n",
    "    else:\n",
    "        realfake_discriminator_B_optimizer.zero_grad()\n",
    "\n",
    "def realfake_discriminator_update(isA=True):\n",
    "    if isA:\n",
    "        realfake_discriminator_A_optimizer.step()\n",
    "    else:\n",
    "        realfake_discriminator_B_optimizer.step()\n",
    "    \n",
    "def realfake_discriminator_forward_loss(fake_noCNNRNN, real, isA=True, gradients_for_discriminator=True):\n",
    "    realfake_discriminator_zero_grad(isA=isA)\n",
    "    realfake_discriminator = realfake_discriminator_A if isA else realfake_discriminator_B\n",
    "    \n",
    "    pred_noCNNRNN = realfake_discriminator(fake_noCNNRNN)\n",
    "    d_loss = torch.mean((0.0 - pred_noCNNRNN) ** 2)\n",
    "    \n",
    "    # if we are running this func for generator gradients, skip real input\n",
    "    # because gradient doesn't flow to generators\n",
    "    if gradients_for_discriminator: \n",
    "        pred_real = realfake_discriminator(real)\n",
    "        d_loss_real = torch.mean((1.0 - pred_real) ** 2)\n",
    "        d_loss = (d_loss + d_loss_real) / 2.0\n",
    "    \n",
    "    return d_loss\n",
    "\n",
    "def compute_identity_loss(fake, real):\n",
    "    B1, C1, T1 = fake.shape\n",
    "    B2, C2, T2 = real.shape\n",
    "    T = min(T1, T2)\n",
    "    fake_common = fake[:,:,:T]\n",
    "    real_common = real[:,:,:T]\n",
    "    identityLoss_noCNNRNN = torch.mean(torch.abs(fake_common - real_common))\n",
    "    return identityLoss_noCNNRNN\n",
    "\n",
    "def compute_embedding_identity_loss(emb1, emb2):\n",
    "    B1, C1, T1 = emb1.shape\n",
    "    B2, C2, T2 = emb2.shape\n",
    "    T = min(T1, T2)\n",
    "    emb1_common = emb1[:,:,:T]\n",
    "    emb2_common = emb2[:,:,:T]\n",
    "    emb1_common = emb1_common / torch.norm(emb1_common, p=2)\n",
    "    emb2_common = emb2_common / torch.norm(emb2_common, p=2)\n",
    "    emb_loss = -torch.sum(emb1_common * emb2_common) # negative cosine similarity as the embedding loss\n",
    "    return emb_loss \n",
    "\n",
    "def compute_cycle_loss(fake, real):\n",
    "    B1, C1, T1 = fake.shape\n",
    "    B2, C2, T2 = real.shape\n",
    "    T = min(T1, T2)\n",
    "    fake_common = fake[:,:,:T]\n",
    "    real_common = real[:,:,:T]\n",
    "    cycleLoss_noCNNRNN = torch.mean(torch.abs(fake_common - real_common))\n",
    "    return cycleLoss_noCNNRNN\n",
    "\n",
    "def generate(voice, isToA=True):\n",
    "    embedding = encoder_noCNNRNN(voice)    \n",
    "    decoder = decoder_2A_noCNNRNN if isToA else decoder_2B_noCNNRNN\n",
    "    fake = decoder(embedding, encoder_noCNNRNN.downsample2_forshape.shape)\n",
    "    return fake\n",
    "    \n",
    "\n",
    "def validation_for_A_dir(save_dir):\n",
    "    num_mcep = 24\n",
    "    sampling_rate = 16000\n",
    "    frame_period = 5.0\n",
    "    n_frames = 128\n",
    "\n",
    "    print(\"Generating Validation Data B from A...\")\n",
    "    for file in os.listdir(validation_A_dir):\n",
    "        filePath = os.path.join(validation_A_dir, file)\n",
    "        wav, _ = librosa.load(filePath, sr=sampling_rate, mono=True)\n",
    "        wav = preprocess.wav_padding(wav=wav,\n",
    "                                     sr=sampling_rate,\n",
    "                                     frame_period=frame_period,\n",
    "                                     multiple=4)\n",
    "        f0, timeaxis, sp, ap = preprocess.world_decompose(\n",
    "            wav=wav, fs=sampling_rate, frame_period=frame_period)\n",
    "        f0_converted = preprocess.pitch_conversion(f0=f0,\n",
    "                                                   mean_log_src=log_f0s_mean_A,\n",
    "                                                   std_log_src=log_f0s_std_A,\n",
    "                                                   mean_log_target=log_f0s_mean_B,\n",
    "                                                   std_log_target=log_f0s_std_B)\n",
    "        coded_sp = preprocess.world_encode_spectral_envelop(\n",
    "            sp=sp, fs=sampling_rate, dim=num_mcep)\n",
    "        coded_sp_transposed = coded_sp.T\n",
    "        coded_sp_norm = (coded_sp_transposed - coded_sps_A_mean) / coded_sps_A_std\n",
    "        coded_sp_norm = np.array([coded_sp_norm])\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            coded_sp_norm = torch.from_numpy(coded_sp_norm).cuda().float()\n",
    "        else:\n",
    "            coded_sp_norm = torch.from_numpy(coded_sp_norm).float()\n",
    "\n",
    "        coded_sp_converted_norm = generate(coded_sp_norm, isToA=False)\n",
    "        coded_sp_converted_norm = coded_sp_converted_norm.cpu().detach().numpy()\n",
    "        coded_sp_converted_norm = np.squeeze(coded_sp_converted_norm)\n",
    "        coded_sp_converted = coded_sp_converted_norm * coded_sps_B_std + coded_sps_B_mean\n",
    "        coded_sp_converted = coded_sp_converted.T\n",
    "        coded_sp_converted = np.ascontiguousarray(coded_sp_converted)\n",
    "        decoded_sp_converted = preprocess.world_decode_spectral_envelop(\n",
    "            coded_sp=coded_sp_converted, fs=sampling_rate)\n",
    "        wav_transformed = preprocess.world_speech_synthesis(f0=f0_converted,\n",
    "                                                            decoded_sp=decoded_sp_converted,\n",
    "                                                            ap=ap,\n",
    "                                                            fs=sampling_rate,\n",
    "                                                            frame_period=frame_period)\n",
    "        librosa.output.write_wav(path=os.path.join(save_dir, os.path.basename(file)),\n",
    "                                 y=wav_transformed,\n",
    "                                 sr=sampling_rate)    \n",
    "\n",
    "def val_file_to_sp(filepath, isA=True):\n",
    "    num_mcep = 24\n",
    "    sampling_rate = 16000\n",
    "    frame_period = 5.0\n",
    "    n_frames = 128\n",
    "    \n",
    "    wav, _ = librosa.load(filepath, sr=sampling_rate, mono=True)\n",
    "    wav = preprocess.wav_padding(wav=wav,\n",
    "                                 sr=sampling_rate,\n",
    "                                 frame_period=frame_period,\n",
    "                                 multiple=4)\n",
    "    f0, timeaxis, sp, ap = preprocess.world_decompose(\n",
    "        wav=wav, fs=sampling_rate, frame_period=frame_period)\n",
    "    \n",
    "    coded_sp = preprocess.world_encode_spectral_envelop(\n",
    "        sp=sp, fs=sampling_rate, dim=num_mcep)\n",
    "    coded_sp_transposed = coded_sp.T\n",
    "    coded_sp_norm = (coded_sp_transposed - coded_sps_A_mean) / coded_sps_A_std if isA else \\\n",
    "                    (coded_sp_transposed - coded_sps_B_mean) / coded_sps_B_std\n",
    "    coded_sp_norm = np.array([coded_sp_norm])\n",
    "    coded_sp_norm = torch.from_numpy(coded_sp_norm).float()\n",
    "    \n",
    "    return coded_sp_norm, f0, ap\n",
    "\n",
    "def val_convert_f0(f0, isAtoB=True):\n",
    "    if isAtoB:\n",
    "        return preprocess.pitch_conversion(f0=f0,\n",
    "                                           mean_log_src=log_f0s_mean_A,\n",
    "                                           std_log_src=log_f0s_std_A,\n",
    "                                           mean_log_target=log_f0s_mean_B,\n",
    "                                           std_log_target=log_f0s_std_B)\n",
    "    else:\n",
    "        return preprocess.pitch_conversion(f0=f0,\n",
    "                                           mean_log_src=log_f0s_mean_B,\n",
    "                                           std_log_src=log_f0s_std_B,\n",
    "                                           mean_log_target=log_f0s_mean_A,\n",
    "                                           std_log_target=log_f0s_std_A)\n",
    "\n",
    "def val_sp_to_file(output_file, coded_sp_converted_norm, f0_converted, ap, isA=True):\n",
    "    num_mcep = 24\n",
    "    sampling_rate = 16000\n",
    "    frame_period = 5.0\n",
    "    n_frames = 128\n",
    "    \n",
    "    coded_sp_converted_norm = np.squeeze(coded_sp_converted_norm)\n",
    "    coded_sp_converted = coded_sp_converted_norm * coded_sps_A_std + coded_sps_A_mean if isA else \\\n",
    "                         coded_sp_converted_norm * coded_sps_B_std + coded_sps_B_mean\n",
    "    coded_sp_converted = coded_sp_converted.T\n",
    "    coded_sp_converted = np.ascontiguousarray(coded_sp_converted)\n",
    "    decoded_sp_converted = preprocess.world_decode_spectral_envelop(\n",
    "        coded_sp=coded_sp_converted, fs=sampling_rate)\n",
    "    wav_transformed = preprocess.world_speech_synthesis(f0=f0_converted,\n",
    "                                                        decoded_sp=decoded_sp_converted,\n",
    "                                                        ap=ap,\n",
    "                                                        fs=sampling_rate,\n",
    "                                                        frame_period=frame_period)\n",
    "    librosa.output.write_wav(path=output_file,\n",
    "                             y=wav_transformed,\n",
    "                             sr=sampling_rate)\n",
    "\n",
    "def validation_for_B_dir(save_dir):\n",
    "\n",
    "    print(\"Generating Validation Data A from B...\")\n",
    "    for file in os.listdir(validation_B_dir):\n",
    "        filePath = os.path.join(validation_B_dir, file)\n",
    "        coded_sp_norm, f0, ap = val_file_to_sp(filepath, isA=False)\n",
    "        \n",
    "        coded_sp_converted_norm = generate(coded_sp_norm.cuda(), isToA=True).cpu().detach().numpy()\n",
    "        f0 = val_convert_f0(f0, isAtoB=False)\n",
    "\n",
    "        output_file = os.path.join(save_dir, os.path.basename(file))\n",
    "        val_sp_to_file(output_file, coded_sp_converted_norm, f0, ap, isA=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"model3_supervise_converge\"\n",
    "MODEL_DIR = \"./experiments/\" + MODEL_NAME\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)\n",
    "file_name = MODEL_DIR + '/log.txt'\n",
    "\n",
    "start_decay = 10\n",
    "num_epochs = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Validation Data B from A...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/0DL/finalproj/Pytorch-CycleGAN-VC2/preprocess.py:114: RuntimeWarning: divide by zero encountered in log\n",
      "  std_log_src * std_log_target + mean_log_target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tLoss/ Cycle: 0.7600\tIdentity: 0.7625\tSupervise: 0.7668\tEmb_Identity: -0.9708\tRF_d: 0.0486\n",
      "Generating Validation Data B from A...\n",
      "Epoch: 10\tLoss/ Cycle: 0.8069\tIdentity: 0.8126\tSupervise: 0.8027\tEmb_Identity: -0.9994\tRF_d: 0.0339\n",
      "Generating Validation Data B from A...\n",
      "Epoch: 20\tLoss/ Cycle: 0.7620\tIdentity: 0.8374\tSupervise: 0.8225\tEmb_Identity: -0.9996\tRF_d: 0.0290\n",
      "Generating Validation Data B from A...\n",
      "Epoch: 30\tLoss/ Cycle: 0.7959\tIdentity: 0.7962\tSupervise: 0.7830\tEmb_Identity: -0.9996\tRF_d: 0.0246\n",
      "Generating Validation Data B from A...\n",
      "Epoch: 40\tLoss/ Cycle: 0.8341\tIdentity: 0.7795\tSupervise: 0.7707\tEmb_Identity: -0.9957\tRF_d: 0.0238\n",
      "Generating Validation Data B from A...\n",
      "Epoch: 50\tLoss/ Cycle: 0.7290\tIdentity: 0.7840\tSupervise: 0.7879\tEmb_Identity: -0.9994\tRF_d: 0.0262\n",
      "Generating Validation Data B from A...\n",
      "Epoch: 60\tLoss/ Cycle: 0.6778\tIdentity: 0.8151\tSupervise: 0.8619\tEmb_Identity: -0.9995\tRF_d: 0.0193\n",
      "Generating Validation Data B from A...\n",
      "Epoch: 70\tLoss/ Cycle: 0.6295\tIdentity: 0.7638\tSupervise: 0.8025\tEmb_Identity: -0.9994\tRF_d: 0.0199\n",
      "Generating Validation Data B from A...\n",
      "Epoch: 80\tLoss/ Cycle: 0.5693\tIdentity: 0.5633\tSupervise: 0.7466\tEmb_Identity: -0.9995\tRF_d: 0.0311\n",
      "Generating Validation Data B from A...\n",
      "Epoch: 90\tLoss/ Cycle: 0.6308\tIdentity: 0.7982\tSupervise: 0.7955\tEmb_Identity: -0.9981\tRF_d: 0.0510\n",
      "Generating Validation Data B from A...\n",
      "Epoch: 100\tLoss/ Cycle: 0.4673\tIdentity: 0.4670\tSupervise: 0.7717\tEmb_Identity: -0.9990\tRF_d: 0.0499\n",
      "Generating Validation Data B from A...\n",
      "Epoch: 110\tLoss/ Cycle: 0.4099\tIdentity: 0.3983\tSupervise: 0.7110\tEmb_Identity: -0.9983\tRF_d: 0.0456\n",
      "Generating Validation Data B from A...\n",
      "Epoch: 120\tLoss/ Cycle: 0.5293\tIdentity: 0.5036\tSupervise: 0.7937\tEmb_Identity: -0.9988\tRF_d: 0.0394\n",
      "Generating Validation Data B from A...\n",
      "Epoch: 130\tLoss/ Cycle: 0.4964\tIdentity: 0.4656\tSupervise: 0.8155\tEmb_Identity: -0.9983\tRF_d: 0.0392\n",
      "Generating Validation Data B from A...\n",
      "Epoch: 140\tLoss/ Cycle: 0.4558\tIdentity: 0.3648\tSupervise: 0.6148\tEmb_Identity: -0.9978\tRF_d: 0.0683\n",
      "Generating Validation Data B from A...\n",
      "Epoch: 150\tLoss/ Cycle: 0.5129\tIdentity: 0.4493\tSupervise: 0.7474\tEmb_Identity: -0.9980\tRF_d: 0.0433\n",
      "Generating Validation Data B from A...\n",
      "Epoch: 160\tLoss/ Cycle: 0.4946\tIdentity: 0.3921\tSupervise: 0.6645\tEmb_Identity: -0.9978\tRF_d: 0.0429\n",
      "Generating Validation Data B from A...\n",
      "Epoch: 170\tLoss/ Cycle: 0.4864\tIdentity: 0.3930\tSupervise: 0.6236\tEmb_Identity: -0.9980\tRF_d: 0.0679\n",
      "Generating Validation Data B from A...\n",
      "Epoch: 180\tLoss/ Cycle: 0.5060\tIdentity: 0.3946\tSupervise: 0.6140\tEmb_Identity: -0.9977\tRF_d: 0.0475\n",
      "Generating Validation Data B from A...\n",
      "Epoch: 190\tLoss/ Cycle: 0.5241\tIdentity: 0.3989\tSupervise: 0.5510\tEmb_Identity: -0.9975\tRF_d: 0.0636\n",
      "Generating Validation Data B from A...\n",
      "Epoch: 200\tLoss/ Cycle: 0.5022\tIdentity: 0.3983\tSupervise: 0.5614\tEmb_Identity: -0.9978\tRF_d: 0.0490\n",
      "Generating Validation Data B from A...\n",
      "Epoch: 210\tLoss/ Cycle: 0.3512\tIdentity: 0.3137\tSupervise: 0.4386\tEmb_Identity: -0.9972\tRF_d: 0.0617\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, num_epochs):\n",
    "    start_time_epoch = time.time()\n",
    "\n",
    "    # Constants\n",
    "    cycle_loss_lambda = 10\n",
    "    identity_loss_lambda = 5\n",
    "    #if epoch>20:#\n",
    "        #cycle_loss_lambda = 15#\n",
    "        #identity_loss_lambda = 0#\n",
    "\n",
    "    # Preparing Dataset\n",
    "    n_samples = len(dataset_A)\n",
    "\n",
    "    dataset = trainingDataset_Paired(datasetA=dataset_A,\n",
    "                              datasetB=dataset_B, max_frames=400)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                               batch_size=mini_batch_size,\n",
    "                                               shuffle=True,\n",
    "                                               drop_last=False)\n",
    "\n",
    "    for i, (real_A, real_B) in enumerate(train_loader):\n",
    "\n",
    "        num_iterations = (n_samples // mini_batch_size) * epoch + i\n",
    "        # print(\"iteration no: \", num_iterations, epoch)\n",
    "\n",
    "        if num_iterations > 10000:\n",
    "            identity_loss_lambda = 0\n",
    "        if num_iterations > start_decay:\n",
    "            for generator_optimizer in generators_optimizers:\n",
    "                adjust_lr_rate(\n",
    "                    generator_optimizer, name='generator')\n",
    "                adjust_lr_rate(\n",
    "                    generator_optimizer, name='discriminator')\n",
    "        \n",
    "        real_A = real_A.to(device).float()\n",
    "        real_B = real_B.to(device).float()\n",
    "        \n",
    "        for (real1, real2, real1_is_A) in [(real_A, real_B, True), (real_B, real_A, False)]:\n",
    "            real2_is_A = not real1_is_A\n",
    "            \n",
    "            reset_grad();\n",
    "            \n",
    "            # ----------------------------------------------------------------\n",
    "            # First compute embeddings for A and B\n",
    "            # real_A and real_B are saying the same sentence\n",
    "            # compute embedding identity loss\n",
    "            embedding2 = encoders_forward(real2) # must execute 2 before 1 because we want decoder shape to be right\n",
    "            embedding1 = encoders_forward(real1)\n",
    "            embedding_identity_loss = compute_embedding_identity_loss(embedding1, embedding2)\n",
    "            \n",
    "            # ----------------------------------------------------------------\n",
    "            # full forward pass and compute loss for improving generators only\n",
    "            # then backward and update generators\n",
    "            \n",
    "#             # adversarial A/B loss\n",
    "#             adversarial_AB_loss = AB_discriminator_forward_loss(embeddings1, \n",
    "#                                                                 isA=real1_is_A, \n",
    "#                                                                 gradients_for_discriminator=False)\n",
    "            \n",
    "            # identity loss\n",
    "            fake11 = decoders_forward(embedding1, isToA=real1_is_A)\n",
    "            identity_loss = compute_identity_loss(fake11, real1)\n",
    "            \n",
    "            # adversarial real/fake loss\n",
    "            fake12 = decoders_forward(embedding1, isToA=real2_is_A)\n",
    "            adversarial_RealFake_loss = realfake_discriminator_forward_loss(fake12, real2, isA=real2_is_A,\n",
    "                                                                            gradients_for_discriminator=False)\n",
    "            \n",
    "            # supervise loss\n",
    "            supervise_loss = compute_cycle_loss(fake12, real2)\n",
    "            \n",
    "            # cycle consistency loss\n",
    "            embedding12 = encoders_forward(fake12)\n",
    "            fake121 = decoders_forward(embedding12, isToA=real1_is_A)\n",
    "            cycle_loss = compute_cycle_loss(fake121, real1)\n",
    "            \n",
    "            # compute total generator loss\n",
    "            total_generator_loss = embedding_identity_loss \\\n",
    "                                 + 0.1 * identity_loss \\\n",
    "                                 + supervise_loss \\\n",
    "                                 + adversarial_RealFake_loss \\\n",
    "                                 + 0.1 * cycle_loss\n",
    "            generator_cycle_loss_store.append(cycle_loss.item())\n",
    "            generator_identity_loss_store.append(identity_loss.item())\n",
    "            generator_supervise_loss_store.append(supervise_loss.item())\n",
    "            embedding_loss_store.append(embedding_identity_loss.item())\n",
    "            \n",
    "            # backward and update generators only\n",
    "            total_generator_loss.backward()\n",
    "            generators_update()\n",
    "            \n",
    "            # --------------------------------------------\n",
    "            # detach embeddings and fakes12 tensors from autograd\n",
    "            # then forward and backward the discriminators only on detached embeddings and fakes12 tensors\n",
    "            # gradients won't flow back to encoders or decoders\n",
    "            \n",
    "#             AB_discriminator_zero_grad()\n",
    "# #             embeddings1 = (embeddings1[0].detach(), embeddings1[1].detach(), embeddings1[2].detach())\n",
    "#             embeddings1 = (None, None, embeddings1[2].detach())\n",
    "#             d_AB_loss = AB_discriminator_forward_loss(embeddings1, \n",
    "#                                                       isA=real1_is_A, \n",
    "#                                                       gradients_for_discriminator=True)\n",
    "#             d_AB_loss.backward()\n",
    "#             AB_discriminator_update()\n",
    "            \n",
    "            realfake_discriminator_zero_grad(isA=real2_is_A)\n",
    "            fake12 = fake12.detach()\n",
    "            d_RealFake_loss = realfake_discriminator_forward_loss(fake12, real2, isA=real2_is_A,\n",
    "                                                                  gradients_for_discriminator=True)\n",
    "            if d_RealFake_loss.item() > 0.1: # only update RF discriminator if generator is good enough\n",
    "                d_RealFake_loss.backward()\n",
    "                realfake_discriminator_update(isA=real2_is_A)\n",
    "\n",
    "            RF_discriminator_loss_store.append(d_RealFake_loss.item())\n",
    "    \n",
    "    # end of epoch\n",
    "    if epoch % 10 == 0:\n",
    "        save_dir = \"/home/frank/Documents/experiments/\"+MODEL_NAME+\"/\"+str(epoch)\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        validation_for_A_dir(save_dir)\n",
    "        store_to_file_str = \\\n",
    "            \"Epoch: {}\\t\".format(epoch) + \\\n",
    "            \"Loss/ Cycle: {:.4f}\\t\".format(generator_cycle_loss_store[-1]) + \\\n",
    "            \"Identity: {:.4f}\\t\".format(generator_identity_loss_store[-1]) + \\\n",
    "            \"Supervise: {:.4f}\\t\".format(generator_supervise_loss_store[-1]) + \\\n",
    "            \"Emb_Identity: {:.4f}\\t\".format(embedding_loss_store[-1]) + \\\n",
    "            \"RF_d: {:.4f}\".format(RF_discriminator_loss_store[-1])\n",
    "        print(store_to_file_str)\n",
    "        store_to_file(store_to_file_str)\n",
    "\n",
    "        # Save the Entire model\n",
    "        saveModelCheckPoint(epoch, save_dir+\"/checkpoint\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"/home/frank/Documents/experiments/\"+MODEL_NAME+\"/150test/B2A\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "validation_for_B_dir(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"/home/frank/Documents/experiments/\"+MODEL_NAME+\"/doubletest\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "file = \"200007.wav\"\n",
    "\n",
    "filepath = os.path.join(validation_A_dir, file)\n",
    "real_A, f0_A, ap_A = val_file_to_sp(filepath, isA=True)\n",
    "f0_AB = val_convert_f0(f0_A, isAtoB=True)\n",
    "\n",
    "filepath = os.path.join(validation_B_dir, file)\n",
    "real_B, f0_B, ap_B = val_file_to_sp(filepath, isA=False)\n",
    "f0_BA = val_convert_f0(f0_B, isAtoB=False)\n",
    "\n",
    "#print(real_A - real_B)\n",
    "# Now we have real_A, real_B, f0_A, f0_B, f0_AB, f0_BA\n",
    "\n",
    "emb_A = encoders_forward(real_A.cuda())\n",
    "fakeAA = decoders_forward(emb_A, isToA=True)\n",
    "fakeAB = decoders_forward(emb_A, isToA=False)\n",
    "emb_B = encoders_forward(real_B.cuda())\n",
    "fakeBB = decoders_forward(emb_B, isToA=False)\n",
    "fakeBA = decoders_forward(emb_B, isToA=True)\n",
    "output_file = os.path.join(save_dir, \"07_AA.wav\")\n",
    "sps = fakeAA.cpu().detach().numpy()\n",
    "val_sp_to_file(output_file, sps, f0_A, ap_A, isA=True)\n",
    "val_sp_to_file(os.path.join(save_dir, \"07_AB.wav\"),  fakeAB.cpu().detach().numpy(), f0_AB, ap_A, isA=False)\n",
    "val_sp_to_file(os.path.join(save_dir, \"07_BB.wav\"),  fakeBB.cpu().detach().numpy(), f0_B, ap_B, isA=False)\n",
    "val_sp_to_file(os.path.join(save_dir, \"07_BA.wav\"),  fakeBA.cpu().detach().numpy(), f0_BA, ap_B, isA=True)\n",
    "\n",
    "val_sp_to_file(os.path.join(save_dir, \"07_realA.wav\"),  real_A.numpy(), f0_A, ap_A, isA=True)\n",
    "val_sp_to_file(os.path.join(save_dir, \"07_realB.wav\"),  real_B.numpy(), f0_B, ap_B, isA=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_A_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
