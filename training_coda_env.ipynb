{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import time\n",
    "import librosa\n",
    "import pickle\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "import preprocess\n",
    "from trainingDataset import trainingDataset\n",
    "from model_VC2 import Generator, Discriminator\n",
    "import torch.utils.tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "class GLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GLU, self).__init__()\n",
    "        # Custom Implementation because the Voice Conversion Cycle GAN\n",
    "        # paper assumes GLU won't reduce the dimension of tensor by 2.\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input * torch.sigmoid(input)\n",
    "\n",
    "\n",
    "class up_2Dsample(nn.Module):\n",
    "    def __init__(self, upscale_factor=2):\n",
    "        super(up_2Dsample, self).__init__()\n",
    "        self.scale_factor = upscale_factor\n",
    "\n",
    "    def forward(self, input):\n",
    "        h = input.shape[2]\n",
    "        w = input.shape[3]\n",
    "        new_size = [h * self.scale_factor, w * self.scale_factor]\n",
    "        return F.interpolate(input,new_size)\n",
    "       \n",
    "\n",
    "class PixelShuffle(nn.Module):\n",
    "    def __init__(self, upscale_factor=2):\n",
    "        super(PixelShuffle, self).__init__()\n",
    "        # Custom Implementation because PyTorch PixelShuffle requires,\n",
    "        # 4D input. Whereas, in this case we have have 3D array\n",
    "        self.upscale_factor = upscale_factor\n",
    "\n",
    "    def forward(self, input):\n",
    "        n = input.shape[0]\n",
    "        c_out = input.shape[1] // self.upscale_factor\n",
    "        w_new = input.shape[2] * self.upscale_factor\n",
    "        return input.view(n, c_out, w_new)\n",
    "\n",
    "\n",
    "class ResidualLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super(ResidualLayer, self).__init__()\n",
    "        self.conv1d_layer = nn.Sequential(nn.Conv1d(in_channels=in_channels,\n",
    "                                                    out_channels=out_channels,\n",
    "                                                    kernel_size=kernel_size,\n",
    "                                                    stride=1,\n",
    "                                                    padding=padding),\n",
    "                                          nn.InstanceNorm1d(num_features=out_channels,\n",
    "                                                            affine=True))\n",
    "\n",
    "        self.conv_layer_gates = nn.Sequential(nn.Conv1d(in_channels=in_channels,\n",
    "                                                        out_channels=out_channels,\n",
    "                                                        kernel_size=kernel_size,\n",
    "                                                        stride=1,\n",
    "                                                        padding=padding),\n",
    "                                              nn.InstanceNorm1d(num_features=out_channels,\n",
    "                                                                affine=True))\n",
    "\n",
    "        self.conv1d_out_layer = nn.Sequential(nn.Conv1d(in_channels=out_channels,\n",
    "                                                        out_channels=in_channels,\n",
    "                                                        kernel_size=kernel_size,\n",
    "                                                        stride=1,\n",
    "                                                        padding=padding),\n",
    "                                              nn.InstanceNorm1d(num_features=in_channels,\n",
    "                                                                affine=True))\n",
    "\n",
    "    def forward(self, input):\n",
    "        #print(\"input size: \", input.size())\n",
    "        h1_norm = self.conv1d_layer(input)\n",
    "        h1_gates_norm = self.conv_layer_gates(input)\n",
    "\n",
    "        # GLU\n",
    "        h1_glu = h1_norm * torch.sigmoid(h1_gates_norm)\n",
    "\n",
    "        h2_norm = self.conv1d_out_layer(h1_glu)\n",
    "        return input + h2_norm\n",
    "\n",
    "\n",
    "class downSample_Generator(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super(downSample_Generator, self).__init__()\n",
    "\n",
    "        self.convLayer = nn.Sequential(nn.Conv2d(in_channels=in_channels,\n",
    "                                                 out_channels=out_channels,\n",
    "                                                 kernel_size=kernel_size,\n",
    "                                                 stride=stride,\n",
    "                                                 padding=padding),\n",
    "                                       nn.InstanceNorm2d(num_features=out_channels,\n",
    "                                                         affine=True))\n",
    "        self.convLayer_gates = nn.Sequential(nn.Conv2d(in_channels=in_channels,\n",
    "                                                       out_channels=out_channels,\n",
    "                                                       kernel_size=kernel_size,\n",
    "                                                       stride=stride,\n",
    "                                                       padding=padding),\n",
    "                                             nn.InstanceNorm2d(num_features=out_channels,\n",
    "                                                               affine=True))\n",
    "\n",
    "    def forward(self, input):\n",
    "        a = self.convLayer(input)\n",
    "        b = self.convLayer_gates(input)\n",
    "        return self.convLayer(input) * torch.sigmoid(self.convLayer_gates(input))\n",
    "\n",
    "\n",
    "class upSample_Generator(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super(upSample_Generator, self).__init__()\n",
    "\n",
    "        self.convLayer = nn.Sequential(nn.Conv2d(in_channels=in_channels,\n",
    "                                                 out_channels=out_channels,\n",
    "                                                 kernel_size=kernel_size,\n",
    "                                                 stride=stride,\n",
    "                                                 padding=padding),\n",
    "                                       #PixelShuffle(upscale_factor=2),\n",
    "                                       up_2Dsample(upscale_factor=2),\n",
    "        nn.InstanceNorm2d(num_features=out_channels, affine=True))\n",
    "        self.convLayer_gates = nn.Sequential(nn.Conv2d(in_channels=in_channels,\n",
    "                                                       out_channels=out_channels,\n",
    "                                                       kernel_size=kernel_size,\n",
    "                                                       stride=stride,\n",
    "                                                       padding=padding),\n",
    "                                             #PixelShuffle(upscale_factor=2),\n",
    "                                             up_2Dsample(upscale_factor=2),\n",
    "                                             nn.InstanceNorm2d(num_features=out_channels,\n",
    "                                                               affine=True))\n",
    "    def forward(self, input):        \n",
    "        return self.convLayer(input) * torch.sigmoid(self.convLayer_gates(input))\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1,\n",
    "                               out_channels=128,\n",
    "                               kernel_size=[5,15],\n",
    "                               stride=1,\n",
    "                               padding=[2,7])\n",
    "\n",
    "        self.conv1_gates = nn.Conv2d(in_channels=1,\n",
    "                               out_channels=128,\n",
    "                               kernel_size=[5,15],\n",
    "                               stride=1,\n",
    "                               padding=[2,7])\n",
    "\n",
    "        # Downsample Layer\n",
    "        self.downSample1 = downSample_Generator(in_channels=128,\n",
    "                                                out_channels=256,\n",
    "                                                kernel_size=5,\n",
    "                                                stride=2,\n",
    "                                                padding=2)\n",
    "\n",
    "        self.downSample2 = downSample_Generator(in_channels=256,\n",
    "                                                out_channels=512,\n",
    "                                                kernel_size=5,\n",
    "                                                stride=2,\n",
    "                                                padding=2)\n",
    "        #reshape\n",
    "        self.conv2 = nn.Conv1d(in_channels=3072,\n",
    "                               out_channels=512,\n",
    "                               kernel_size=1,\n",
    "                               stride=1)\n",
    "\n",
    "        # Residual Blocks\n",
    "        self.residualLayer1 = ResidualLayer(in_channels=512,\n",
    "                                            out_channels=1024,\n",
    "                                            kernel_size=3,\n",
    "                                            stride=1,\n",
    "                                            padding=1)\n",
    "        self.residualLayer2 = ResidualLayer(in_channels=512,\n",
    "                                            out_channels=1024,\n",
    "                                            kernel_size=3,\n",
    "                                            stride=1,\n",
    "                                            padding=1)\n",
    "        self.residualLayer3 = ResidualLayer(in_channels=512,\n",
    "                                            out_channels=1024,\n",
    "                                            kernel_size=3,\n",
    "                                            stride=1,\n",
    "                                            padding=1)\n",
    "    def forward(self, input):\n",
    "        # GLU\n",
    "        input = input.unsqueeze(1)\n",
    "        conv1 = self.conv1(input) * torch.sigmoid(self.conv1_gates(input))\n",
    "        # print(\"shape of conv1, \", conv1.size())\n",
    "        downsample1 = self.downSample1(conv1)\n",
    "        # print(\"shape of downsample1, \", downsample1.size())\n",
    "        self.downsample2_forshape = self.downSample2(downsample1)\n",
    "        downsample3 = self.downsample2_forshape.view([self.downsample2_forshape.shape[0],-1,self.downsample2_forshape.shape[3]])\n",
    "        downsample3 = self.conv2(downsample3)\n",
    "        # print(\"shape of downsample3, \", downsample3.size())\n",
    "        residual_layer_1 = self.residualLayer1(downsample3)\n",
    "        residual_layer_2 = self.residualLayer2(residual_layer_1)\n",
    "        # print(\"shape of residual_layer_2, \", residual_layer_2.size())\n",
    "        residual_layer_3 = self.residualLayer3(residual_layer_2)\n",
    "        # print(\"shape of residual_layer_3, \", residual_layer_3.size())\n",
    "        return residual_layer_3\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.residualLayer4 = ResidualLayer(in_channels=512,\n",
    "                                            out_channels=1024,\n",
    "                                            kernel_size=3,\n",
    "                                            stride=1,\n",
    "                                            padding=1)\n",
    "        self.residualLayer5 = ResidualLayer(in_channels=512,\n",
    "                                            out_channels=1024,\n",
    "                                            kernel_size=3,\n",
    "                                            stride=1,\n",
    "                                            padding=1)\n",
    "        self.residualLayer6 = ResidualLayer(in_channels=512,\n",
    "                                            out_channels=1024,\n",
    "                                            kernel_size=3,\n",
    "                                            stride=1,\n",
    "                                            padding=1)\n",
    "        #reshape\n",
    "        self.conv3 = nn.Conv1d(in_channels=512,\n",
    "                               out_channels=3072,\n",
    "                               kernel_size=1,\n",
    "                               stride=1)\n",
    "\n",
    "\n",
    "        # UpSample Layer\n",
    "        self.upSample1 = upSample_Generator(in_channels=512,\n",
    "                                            out_channels=1024,\n",
    "                                            kernel_size=5,\n",
    "                                            stride=1,\n",
    "                                            padding=2)\n",
    "        \n",
    "        self.upSample2 = upSample_Generator(in_channels=1024,\n",
    "                                            out_channels=512,\n",
    "                                            kernel_size=5,\n",
    "                                            stride=1,\n",
    "                                            padding=2)\n",
    "\n",
    "        self.lastConvLayer = nn.Conv2d(in_channels=512,\n",
    "                                       out_channels=1,\n",
    "                                       kernel_size=[5,15],\n",
    "                                       stride=1,\n",
    "                                       padding=[2,7])\n",
    "\n",
    "    def forward(self, input, shapes):\n",
    "        # GLU\n",
    "        residual_layer_4 = self.residualLayer4(input)\n",
    "        residual_layer_5 = self.residualLayer5(residual_layer_4)\n",
    "        residual_layer_6 = self.residualLayer6(residual_layer_5)\n",
    "        residual_layer_6 = self.conv3(residual_layer_6)\n",
    "        residual_layer_6 = residual_layer_6.view([shapes[0],shapes[1],shapes[2],shapes[3]])\n",
    "        \n",
    "        upSample_layer_1 = self.upSample1(residual_layer_6)\n",
    "        upSample_layer_2 = self.upSample2(upSample_layer_1)\n",
    "        output = self.lastConvLayer(upSample_layer_2)\n",
    "        output = output.view([output.shape[0],-1,output.shape[3]])\n",
    "        return output\n",
    "\n",
    "\n",
    "class CNN_Discriminator(nn.Module):\n",
    "    def __init__(self, Cin=1024, Tmax=256): # all inputs are padded to Tmax \n",
    "        super(CNN_Discriminator, self).__init__()\n",
    "        self.Tmax = Tmax\n",
    "        Conv_dim = 256\n",
    "        self.conv = nn.Conv1d(in_channels=Cin,\n",
    "                              out_channels=Conv_dim,\n",
    "                              kernel_size=3,\n",
    "                              stride=1)\n",
    "        self.conv_2 = nn.Conv1d(in_channels=Conv_dim,\n",
    "                            out_channels=Conv_dim,\n",
    "                            kernel_size=3,\n",
    "                            stride=1,\n",
    "                            padding=1)\n",
    "        self.conv_3 = nn.Conv1d(in_channels=Conv_dim,\n",
    "                            out_channels=Conv_dim,\n",
    "                            kernel_size=3,\n",
    "                            stride=1,\n",
    "                            padding=1)\n",
    "        self.avgPool = nn.AvgPool1d(kernel_size=Tmax-2)\n",
    "        self.linear = nn.Linear(Conv_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x): # input tensor in shape (B, Cin, Tin)\n",
    "        # print(\"input size at discriminator ,\", x.size())\n",
    "        B, Cin, Tin = x.shape\n",
    "        x = x.unsqueeze(1) # (B, 1, Cin, Tin)\n",
    "        # print(\"input size after unsqueeze ,\", x.size())\n",
    "        Tpad_right = self.Tmax - Tin\n",
    "        padder = nn.ZeroPad2d((0,Tpad_right,0,0))\n",
    "        x = padder(x) # (B, 1, Cin, Tmax)\n",
    "        # print(\"input size after pad ,\", x.size())\n",
    "        x = x.squeeze() # (B, Cin, Tmax)\n",
    "        # print(\"input size before conv ,\", x.size())\n",
    "        x = self.conv(x) # (B, Conv_dim, Tmax-2)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.conv_3(x)\n",
    "        x = self.avgPool(x) # (B, Conv_dim, 1)\n",
    "        x = x.squeeze() # (B, Conv_dim)\n",
    "        x = self.linear(x) # (B, 1)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "class RNN_Discriminator(nn.Module):\n",
    "    def __init__(self, Cin=1024): # all inputs are padded to Tmax \n",
    "        super(RNN_Discriminator, self).__init__()\n",
    "        Conv_dim = 256\n",
    "        self.conv = nn.Conv1d(in_channels=Cin,\n",
    "                              out_channels=Conv_dim,\n",
    "                              kernel_size=3,\n",
    "                              stride=1)\n",
    "        hidden = 256\n",
    "        self.gru = nn.GRU(input_size=Conv_dim, \n",
    "                          hidden_size=hidden,\n",
    "                         num_layers=3)\n",
    "        self.linear = nn.Linear(hidden, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x): # input tensor in shape (B, Cin, Tin)\n",
    "        B, Cin, Tin = x.shape\n",
    "        x = self.conv(x) # (B, Conv_dim, Tin-2)\n",
    "        x = x.permute(2, 0, 1) # (Tin-2, B, Conv_dim)\n",
    "        x, hout = self.gru(x) # hout: (1, B, hidden)\n",
    "        hout = hout.squeeze() # hout: (B, hidden)\n",
    "        hout = self.linear(hout) # (B, 1)\n",
    "        hout = self.sigmoid(hout) # (B, 1)\n",
    "        return hout\n",
    "\n",
    "\n",
    "\n",
    "class DownSample_Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super(DownSample_Discriminator, self).__init__()\n",
    "\n",
    "        self.convLayer = nn.Sequential(nn.Conv2d(in_channels=in_channels,\n",
    "                                                 out_channels=out_channels,\n",
    "                                                 kernel_size=kernel_size,\n",
    "                                                 stride=stride,\n",
    "                                                 padding=padding),\n",
    "                                       nn.InstanceNorm2d(num_features=out_channels,\n",
    "                                                         affine=True))\n",
    "        self.convLayerGates = nn.Sequential(nn.Conv2d(in_channels=in_channels,\n",
    "                                                      out_channels=out_channels,\n",
    "                                                      kernel_size=kernel_size,\n",
    "                                                      stride=stride,\n",
    "                                                      padding=padding),\n",
    "                                            nn.InstanceNorm2d(num_features=out_channels,\n",
    "                                                              affine=True))\n",
    "\n",
    "    def forward(self, input):\n",
    "        # GLU\n",
    "        return self.convLayer(input) * torch.sigmoid(self.convLayerGates(input))\n",
    "        \n",
    "\n",
    "class RealFake_Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RealFake_Discriminator, self).__init__()\n",
    "\n",
    "        self.convLayer1 = nn.Conv2d(in_channels=1,\n",
    "                                    out_channels=128,\n",
    "                                    kernel_size=[3, 3],\n",
    "                                    stride=[1, 1])\n",
    "        self.convLayer1_gates = nn.Conv2d(in_channels=1,\n",
    "                                          out_channels=128,\n",
    "                                          kernel_size=[3, 3],\n",
    "                                          stride=[1, 1])\n",
    "\n",
    "        # Note: Kernel Size have been modified in the PyTorch implementation\n",
    "        # compared to the actual paper, as to retain dimensionality. Unlike,\n",
    "        # TensorFlow, PyTorch doesn't have padding='same', hence, kernel sizes\n",
    "        # were altered to retain the dimensionality after each layer\n",
    "\n",
    "        # DownSample Layer\n",
    "        self.downSample1 = DownSample_Discriminator(in_channels=128,\n",
    "                                                    out_channels=256,\n",
    "                                                    kernel_size=[3, 3],\n",
    "                                                    stride=[2, 2],\n",
    "                                                    padding=0)\n",
    "\n",
    "        self.downSample2 = DownSample_Discriminator(in_channels=256,\n",
    "                                                    out_channels=512,\n",
    "                                                    kernel_size=[3, 3],\n",
    "                                                    stride=[2, 2],\n",
    "                                                    padding=0)\n",
    "\n",
    "        self.downSample3 = DownSample_Discriminator(in_channels=512,\n",
    "                                                    out_channels=1024,\n",
    "                                                    kernel_size=[3, 3],\n",
    "                                                    stride=[2, 2],\n",
    "                                                    padding=0)\n",
    "\n",
    "        self.downSample4 = DownSample_Discriminator(in_channels=1024,\n",
    "                                                    out_channels=1024,\n",
    "                                                    kernel_size=[1, 5],\n",
    "                                                    stride=[1, 1],\n",
    "                                                    padding=[0, 2])\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        self.fc = nn.Linear(in_features=1024,\n",
    "                            out_features=1)\n",
    "\n",
    "        # output Layer\n",
    "        self.output_layer = nn.Conv2d(in_channels=1024,\n",
    "                                      out_channels=1,\n",
    "                                      kernel_size=[1, 3],\n",
    "                                      stride=[1, 1],\n",
    "                                      padding=[0, 1])\n",
    "\n",
    "    def forward(self, input):\n",
    "        # input has shape [batch_size, num_features, time]\n",
    "        # discriminator requires shape [batchSize, 1, num_features, time]\n",
    "        input = input.unsqueeze(1)\n",
    "        # GLU\n",
    "        pad_input = nn.ZeroPad2d((1, 1, 1, 1))\n",
    "        layer1 = self.convLayer1(\n",
    "            pad_input(input)) * torch.sigmoid(self.convLayer1_gates(pad_input(input)))\n",
    "\n",
    "        pad_input = nn.ZeroPad2d((1, 0, 1, 0))\n",
    "        downSample1 = self.downSample1(pad_input(layer1))\n",
    "\n",
    "        pad_input = nn.ZeroPad2d((1, 0, 1, 0))\n",
    "        downSample2 = self.downSample2(pad_input(downSample1))\n",
    "\n",
    "        pad_input = nn.ZeroPad2d((1, 0, 1, 0))\n",
    "        downSample3 = self.downSample3(pad_input(downSample2))\n",
    "\n",
    "        downSample4 = self.downSample4(downSample3)\n",
    "        downSample4 = self.output_layer(downSample4)\n",
    "\n",
    "        downSample4 = downSample4.contiguous().permute(0, 2, 3, 1).contiguous()\n",
    "        # fc = torch.sigmoid(self.fc(downSample3))\n",
    "        # Taking off sigmoid layer to avoid vanishing gradient problem\n",
    "        #fc = self.fc(downSample4)\n",
    "        fc = torch.sigmoid(downSample4)\n",
    "        return fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def adjust_lr_rate(optimizer, name='generator'):\n",
    "    global generator_lr, generator_lr_decay, discriminator_lr, discriminator_lr_decay\n",
    "    if name == 'generator':\n",
    "        generator_lr = max(\n",
    "            0., generator_lr - generator_lr_decay)\n",
    "        for param_groups in optimizer.param_groups:\n",
    "            param_groups['lr'] = generator_lr\n",
    "    else:\n",
    "        discriminator_lr = max(\n",
    "            0., discriminator_lr - discriminator_lr_decay)\n",
    "        for param_groups in optimizer.param_groups:\n",
    "            param_groups['lr'] = discriminator_lr\n",
    "\n",
    "def reset_grad():\n",
    "    encoder_noCNN_optimizer.zero_grad()\n",
    "    encoder_noRNN_optimizer.zero_grad()\n",
    "    encoder_noCNNRNN_optimizer.zero_grad()\n",
    "    \n",
    "    decoder_2A_noCNN_optimizer.zero_grad()\n",
    "    decoder_2A_noRNN_optimizer.zero_grad()\n",
    "    decoder_2A_noCNNRNN_optimizer.zero_grad()\n",
    "    decoder_2B_noCNN_optimizer.zero_grad()\n",
    "    decoder_2B_noRNN_optimizer.zero_grad()\n",
    "    decoder_2B_noCNNRNN_optimizer.zero_grad()\n",
    "    \n",
    "    A_realfake_discriminator_optimizer.zero_grad()\n",
    "    B_realfake_discriminator_optimizer.zero_grad()\n",
    "    cnn_discriminator_optimizer.zero_grad()\n",
    "    rnn_discriminator_optimizer.zero_grad()\n",
    "\n",
    "def savePickle(variable, fileName):\n",
    "    with open(fileName, 'wb') as f:\n",
    "        pickle.dump(variable, f)\n",
    "\n",
    "def loadPickleFile(fileName):\n",
    "    with open(fileName, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def store_to_file(doc):\n",
    "    doc = doc + \"\\n\"\n",
    "    with open(file_name, \"a\") as myfile:\n",
    "        myfile.write(doc)\n",
    "\n",
    "def saveModelCheckPoint(self, epoch, PATH):\n",
    "     torch.save({\n",
    "         'epoch': epoch,\n",
    "         'generator_loss_store': generator_loss_store,\n",
    "         'discriminator_loss_store': discriminator_loss_store,\n",
    "         'model_genA2B_state_dict': generator_A2B.state_dict(),\n",
    "         'model_genB2A_state_dict': generator_B2A.state_dict(),\n",
    "         'model_discriminatorA': discriminator_A.state_dict(),\n",
    "         'model_discriminatorB': discriminator_B.state_dict(),\n",
    "         'generator_optimizer': generator_optimizer.state_dict(),\n",
    "         'discriminator_optimizer': discriminator_optimizer.state_dict()\n",
    "     }, PATH)\n",
    "\n",
    "# def loadModel(PATH):\n",
    "#     checkPoint = torch.load(PATH)\n",
    "#     generator_A2B.load_state_dict(\n",
    "#         state_dict=checkPoint['model_genA2B_state_dict'])\n",
    "#     generator_B2A.load_state_dict(\n",
    "#         state_dict=checkPoint['model_genB2A_state_dict'])\n",
    "#     discriminator_A.load_state_dict(\n",
    "#         state_dict=checkPoint['model_discriminatorA'])\n",
    "#     discriminator_B.load_state_dict(\n",
    "#         state_dict=checkPoint['model_discriminatorB'])\n",
    "#     generator_optimizer.load_state_dict(\n",
    "#         state_dict=checkPoint['generator_optimizer'])\n",
    "#     discriminator_optimizer.load_state_dict(\n",
    "#         state_dict=checkPoint['discriminator_optimizer'])\n",
    "#     epoch = int(checkPoint['epoch']) + 1\n",
    "#     generator_loss_store = checkPoint['generator_loss_store']\n",
    "#     discriminator_loss_store = checkPoint['discriminator_loss_store']\n",
    "#     return epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up\n",
    "logf0s_normalization = \"./cache/logf0s_normalization.npz\"\n",
    "mcep_normalization = \"./cache/mcep_normalization.npz\"\n",
    "coded_sps_A_norm = \"./cache/coded_sps_A_norm.pickle\"\n",
    "coded_sps_B_norm = \"./cache/coded_sps_B_norm.pickle\" \n",
    "resume_training_at = \"./cache/model_checkpoint/_CycleGAN_CheckPoint\"\n",
    "validation_A_dir = \"./data/vcc2016_training/evaluation_all/SF1/\" \n",
    "output_A_dir = \"./data/vcc2016_training/converted_sound/SF1\"\n",
    "validation_B_dir = \"./data/vcc2016_training/evaluation_all/TF2/\" \n",
    "output_B_dir = \"./data/vcc2016_training/converted_sound/TF2/\"\n",
    "all_dir = [resume_training_at, validation_A_dir, output_A_dir, validation_B_dir, output_B_dir]\n",
    "# =================================================\n",
    "start_epoch = 0\n",
    "num_epochs = 5000\n",
    "mini_batch_size = 8\n",
    "dataset_A = loadPickleFile(coded_sps_A_norm)\n",
    "dataset_B = loadPickleFile(coded_sps_B_norm)\n",
    "device = torch.device(\n",
    "    'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Speech Parameters\n",
    "logf0s_normalization = np.load(logf0s_normalization)\n",
    "log_f0s_mean_A = logf0s_normalization['mean_A']\n",
    "log_f0s_std_A = logf0s_normalization['std_A']\n",
    "log_f0s_mean_B = logf0s_normalization['mean_B']\n",
    "log_f0s_std_B = logf0s_normalization['std_B']\n",
    "\n",
    "mcep_normalization = np.load(mcep_normalization)\n",
    "coded_sps_A_mean = mcep_normalization['mean_A']\n",
    "coded_sps_A_std = mcep_normalization['std_A']\n",
    "coded_sps_B_mean = mcep_normalization['mean_B']\n",
    "coded_sps_B_std = mcep_normalization['std_B']\n",
    "\n",
    "# Encoder and Decoder\n",
    "encoder_noCNN = Encoder().to(device)\n",
    "encoder_noRNN = Encoder().to(device)\n",
    "encoder_noCNNRNN = Encoder().to(device)\n",
    "decoder_2A_noCNN = Decoder().to(device)\n",
    "decoder_2A_noRNN = Decoder().to(device)\n",
    "decoder_2A_noCNNRNN = Decoder().to(device)\n",
    "decoder_2B_noCNN = Decoder().to(device)\n",
    "decoder_2B_noRNN = Decoder().to(device)\n",
    "decoder_2B_noCNNRNN = Decoder().to(device)\n",
    "\n",
    "# Discriminator\n",
    "cnn_discriminator = CNN_Discriminator(Cin=512, Tmax=256).to(device)\n",
    "rnn_discriminator = RNN_Discriminator(Cin=512).to(device)\n",
    "A_realfake_discriminator = RealFake_Discriminator().to(device)\n",
    "B_realfake_discriminator = RealFake_Discriminator().to(device)\n",
    "\n",
    "# Loss Functions\n",
    "criterion_mse = torch.nn.MSELoss()\n",
    "\n",
    "# Initial learning rates\n",
    "generator_lr = 0.0002\n",
    "discriminator_lr = 0.0001\n",
    "realfake_discriminator_lr = 0.0001\n",
    "\n",
    "# Learning rate decay\n",
    "generator_lr_decay = generator_lr / 200000\n",
    "discriminator_lr_decay = discriminator_lr / 200000\n",
    "\n",
    "# Optimizers\n",
    "def get_optimizer(one_module, learning_rate):\n",
    "    return torch.optim.Adam(list(one_module.parameters()), lr=learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "encoder_noCNN_optimizer = get_optimizer(encoder_noCNN, generator_lr)\n",
    "encoder_noRNN_optimizer = get_optimizer(encoder_noRNN, generator_lr)\n",
    "encoder_noCNNRNN_optimizer = get_optimizer(encoder_noCNNRNN, generator_lr)\n",
    "\n",
    "decoder_2A_noCNN_optimizer = get_optimizer(decoder_2A_noCNN, generator_lr)\n",
    "decoder_2A_noRNN_optimizer = get_optimizer(decoder_2A_noRNN, generator_lr)\n",
    "decoder_2A_noCNNRNN_optimizer = get_optimizer(decoder_2A_noCNNRNN, generator_lr)\n",
    "decoder_2B_noCNN_optimizer = get_optimizer(decoder_2B_noCNN, generator_lr)\n",
    "decoder_2B_noRNN_optimizer = get_optimizer(decoder_2B_noRNN, generator_lr)\n",
    "decoder_2B_noCNNRNN_optimizer = get_optimizer(decoder_2B_noCNNRNN, generator_lr)\n",
    "\n",
    "generators_optimizers = [\n",
    "    encoder_noCNN_optimizer,\n",
    "    encoder_noRNN_optimizer,\n",
    "    encoder_noCNNRNN_optimizer,\n",
    "    decoder_2A_noCNN_optimizer,\n",
    "    decoder_2A_noRNN_optimizer,\n",
    "    decoder_2A_noCNNRNN_optimizer,\n",
    "    decoder_2B_noCNN_optimizer,\n",
    "    decoder_2B_noRNN_optimizer,\n",
    "    decoder_2B_noCNNRNN_optimizer\n",
    "]\n",
    "\n",
    "\n",
    "A_realfake_discriminator_optimizer = get_optimizer(A_realfake_discriminator, realfake_discriminator_lr)\n",
    "B_realfake_discriminator_optimizer = get_optimizer(B_realfake_discriminator, realfake_discriminator_lr)\n",
    "cnn_discriminator_optimizer = get_optimizer(cnn_discriminator, discriminator_lr)\n",
    "rnn_discriminator_optimizer = get_optimizer(rnn_discriminator, discriminator_lr)\n",
    "\n",
    "\n",
    "# To Load save previously saved models\n",
    "#modelCheckpoint = model_checkpoint\n",
    "\n",
    "# Validation set Parameters\n",
    "validation_A_dir = validation_A_dir\n",
    "output_A_dir = output_A_dir\n",
    "validation_B_dir = validation_B_dir\n",
    "output_B_dir = output_B_dir\n",
    "\n",
    "# Storing Discriminatior and Generator Loss\n",
    "generator_loss_store = []\n",
    "discriminator_loss_store = []\n",
    "cnn_discriminator_loss_store = []\n",
    "rnn_discriminator_loss_store = []\n",
    "\n",
    "file_name = 'log_store_non_sigmoid.txt'\n",
    "start_epoch = 0\n",
    "'''\n",
    "if restart_training_at is not None:\n",
    "    # Training will resume from previous checkpoint\n",
    "    start_epoch = loadModel(restart_training_at)\n",
    "    print(\"Training resumed\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generators_reset_grad():\n",
    "    for opt in generators_optimizers:\n",
    "        opt.zero_grad()\n",
    "\n",
    "def generators_update():\n",
    "    for opt in generators_optimizers:\n",
    "        opt.step()\n",
    "\n",
    "def encoders_forward(voices):\n",
    "    voice_noCNN, voice_noRNN, voice_noCNNRNN = voices\n",
    "\n",
    "    embedding_noCNN = encoder_noCNN(voice_noCNN)\n",
    "    embedding_noRNN = encoder_noRNN(voice_noRNN)\n",
    "    embedding_noCNNRNN = encoder_noCNNRNN(voice_noCNNRNN)\n",
    "    embeddings = (embedding_noCNN, embedding_noRNN, embedding_noCNNRNN)\n",
    "    return embeddings\n",
    "\n",
    "def decoders_forward(embeddings, isToA=True):\n",
    "    embedding_noCNN, embedding_noRNN, embedding_noCNNRNN = embeddings\n",
    "    \n",
    "    decoder_noCNN, decoder_noRNN, decoder_noCNNRNN = \\\n",
    "        (decoder_2A_noCNN, decoder_2A_noRNN, decoder_2A_noCNNRNN) if isToA else \\\n",
    "        (decoder_2B_noCNN, decoder_2B_noRNN, decoder_2B_noCNNRNN)\n",
    "    \n",
    "    fake_noCNN = decoder_noCNN(embedding_noCNN, encoder_noCNN.downsample2_forshape.shape)\n",
    "    fake_noRNN = decoder_noRNN(embedding_noRNN, encoder_noRNN.downsample2_forshape.shape)\n",
    "    fake_noCNNRNN = decoder_noCNNRNN(embedding_noCNNRNN, encoder_noCNNRNN.downsample2_forshape.shape)\n",
    "    fakes = (fake_noCNN, fake_noRNN, fake_noCNNRNN)\n",
    "\n",
    "    return fakes\n",
    "\n",
    "def AB_discriminator_zero_grad():\n",
    "    cnn_discriminator_optimizer.zero_grad()\n",
    "    rnn_discriminator_optimizer.zero_grad()\n",
    "\n",
    "def AB_discriminator_update():\n",
    "    cnn_discriminator_optimizer.step()\n",
    "    rnn_discriminator_optimizer.step()\n",
    "\n",
    "def AB_discriminator_forward_loss(embeddings, isA=True, gradients_for_discriminator=True):\n",
    "    emb_noCNN, emb_noRNN, emb_noCNNRNN = embeddings\n",
    "    \n",
    "    true_pred = 0.0 if isA else 1.0\n",
    "    \n",
    "    target_cnn_01 = true_pred if gradients_for_discriminator else 0.5\n",
    "    target_cnn_10 = true_pred if gradients_for_discriminator else true_pred\n",
    "    target_cnn_00 = true_pred if gradients_for_discriminator else 0.5\n",
    "    \n",
    "    target_rnn_01 = true_pred if gradients_for_discriminator else true_pred\n",
    "    target_rnn_10 = true_pred if gradients_for_discriminator else 0.5\n",
    "    target_rnn_00 = true_pred if gradients_for_discriminator else 0.5\n",
    "    \n",
    "    # forward CNN discriminator for all 3 embeddings\n",
    "    pred_noCNN_cnn_discriminator = cnn_discriminator(emb_noCNN)\n",
    "    pred_noRNN_cnn_discriminator = cnn_discriminator(emb_noRNN)\n",
    "    pred_noCNNRNN_cnn_discriminator = cnn_discriminator(emb_noCNNRNN)\n",
    "    \n",
    "    d_loss_cnn_1 = torch.mean((target_cnn_01 - pred_noCNN_cnn_discriminator) ** 2)\n",
    "    d_loss_cnn_2 = torch.mean((target_cnn_10 - pred_noRNN_cnn_discriminator) ** 2)\n",
    "    d_loss_cnn_3 = torch.mean((target_cnn_00 - pred_noCNNRNN_cnn_discriminator) ** 2)\n",
    "        \n",
    "    # forward RNN discriminator for all 3 embeddings\n",
    "    pred_noCNN_rnn_discriminator = rnn_discriminator(emb_noCNN)\n",
    "    pred_noRNN_rnn_discriminator = rnn_discriminator(emb_noRNN)\n",
    "    pred_noCNNRNN_rnn_discriminator = rnn_discriminator(emb_noCNNRNN)\n",
    "    \n",
    "    d_loss_rnn_1 = torch.mean((target_rnn_01 - pred_noCNN_rnn_discriminator) ** 2)\n",
    "    d_loss_rnn_2 = torch.mean((target_rnn_10 - pred_noRNN_rnn_discriminator) ** 2)\n",
    "    d_loss_rnn_3 = torch.mean((target_rnn_00 - pred_noCNNRNN_rnn_discriminator) ** 2)\n",
    "    \n",
    "    # accumulate loss\n",
    "    d_loss = d_loss_cnn_1 + d_loss_cnn_2 + d_loss_cnn_3 \\\n",
    "            +d_loss_rnn_1 + d_loss_rnn_2 + d_loss_rnn_3\n",
    "    \n",
    "    return d_loss\n",
    "\n",
    "def realfake_discriminator_zero_grad(realfake_discriminator_optimizer):\n",
    "    realfake_discriminator_optimizer.zero_grad()\n",
    "\n",
    "def realfake_discriminator_update(realfake_discriminator_optimizer):\n",
    "    realfake_discriminator_optimizer.step()\n",
    "    \n",
    "def realfake_discriminator_forward_loss(fakes, real, realfake_discriminator_optimizer, realfake_discriminator, gradients_for_discriminator=True):\n",
    "    realfake_discriminator_optimizer.zero_grad()\n",
    "    \n",
    "    fake_noCNN, fake_noRNN, fake_noCNNRNN = fakes\n",
    "    \n",
    "    pred_noCNN = realfake_discriminator(fake_noCNN)\n",
    "    pred_noRNN = realfake_discriminator(fake_noRNN)\n",
    "    pred_noCNNRNN = realfake_discriminator(fake_noCNNRNN)\n",
    "    \n",
    "    d_loss_noCNN = torch.mean((0.0 - pred_noCNN) ** 2)\n",
    "    d_loss_noRNN = torch.mean((0.0 - pred_noRNN) ** 2)\n",
    "    d_loss_noCNNRNN = torch.mean((0.0 - pred_noCNNRNN) ** 2)\n",
    "    \n",
    "    d_loss = (d_loss_noCNN + d_loss_noRNN + d_loss_noCNNRNN) / 3.0\n",
    "    \n",
    "    # if we are running this func for generator gradients, skip real input\n",
    "    # because gradient doesn't flow to generators\n",
    "    if gradients_for_discriminator: \n",
    "        pred_real = realfake_discriminator(real)\n",
    "        d_loss_real = torch.mean((1.0 - pred_real) ** 2)\n",
    "        d_loss = (d_loss + d_loss_real) / 2.0\n",
    "    \n",
    "    return d_loss\n",
    "\n",
    "def compute_identity_loss(fakes, real):\n",
    "    fake_noCNN, fake_noRNN, fake_noCNNRNN = fakes\n",
    "    identityLoss_noCNN = torch.mean(torch.abs(real - fake_noCNN))\n",
    "    identityLoss_noRNN = torch.mean(torch.abs(real - fake_noRNN))\n",
    "    identityLoss_noCNNRNN = torch.mean(torch.abs(real - fake_noCNNRNN))\n",
    "    identityLoss = (identityLoss_noCNN + identityLoss_noRNN + identityLoss_noCNNRNN) / 3.0\n",
    "    return identityLoss\n",
    "\n",
    "def compute_cycle_loss(fakes, real):\n",
    "    fake_noCNN, fake_noRNN, fake_noCNNRNN = fakes\n",
    "    cycleLoss_noCNN = torch.mean(torch.abs(real - fake_noCNN))\n",
    "    cycleLoss_noRNN = torch.mean(torch.abs(real - fake_noRNN))\n",
    "    cycleLoss_noCNNRNN = torch.mean(torch.abs(real - fake_noCNNRNN))\n",
    "    cycleLoss = (cycleLoss_noCNN + cycleLoss_noRNN + cycleLoss_noCNNRNN) / 3.0\n",
    "    return cycleLoss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(voice, isToA=True):\n",
    "    embedding = encoder_noCNNRNN(voice)    \n",
    "    decoder = decoder_2A_noCNNRNN if isToA else decoder_2B_noCNNRNN\n",
    "    fake = decoder(embedding, encoder_noCNNRNN.downsample2_forshape.shape)\n",
    "    return fake\n",
    "\n",
    "def validation_for_A_dir(save_dir):\n",
    "    num_mcep = 24\n",
    "    sampling_rate = 16000\n",
    "    frame_period = 5.0\n",
    "    n_frames = 128\n",
    "\n",
    "    print(\"Generating Validation Data B from A...\")\n",
    "    for file in os.listdir(validation_A_dir):\n",
    "        filePath = os.path.join(validation_A_dir, file)\n",
    "        wav, _ = librosa.load(filePath, sr=sampling_rate, mono=True)\n",
    "        wav = preprocess.wav_padding(wav=wav,\n",
    "                                     sr=sampling_rate,\n",
    "                                     frame_period=frame_period,\n",
    "                                     multiple=4)\n",
    "        f0, timeaxis, sp, ap = preprocess.world_decompose(\n",
    "            wav=wav, fs=sampling_rate, frame_period=frame_period)\n",
    "        f0_converted = preprocess.pitch_conversion(f0=f0,\n",
    "                                                   mean_log_src=log_f0s_mean_A,\n",
    "                                                   std_log_src=log_f0s_std_A,\n",
    "                                                   mean_log_target=log_f0s_mean_B,\n",
    "                                                   std_log_target=log_f0s_std_B)\n",
    "        coded_sp = preprocess.world_encode_spectral_envelop(\n",
    "            sp=sp, fs=sampling_rate, dim=num_mcep)\n",
    "        coded_sp_transposed = coded_sp.T\n",
    "        coded_sp_norm = (coded_sp_transposed - coded_sps_A_mean) / coded_sps_A_std\n",
    "        coded_sp_norm = np.array([coded_sp_norm])\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            coded_sp_norm = torch.from_numpy(coded_sp_norm).cuda().float()\n",
    "        else:\n",
    "            coded_sp_norm = torch.from_numpy(coded_sp_norm).float()\n",
    "\n",
    "        coded_sp_converted_norm = generate(coded_sp_norm, isToA=False)\n",
    "        coded_sp_converted_norm = coded_sp_converted_norm.cpu().detach().numpy()\n",
    "        coded_sp_converted_norm = np.squeeze(coded_sp_converted_norm)\n",
    "        coded_sp_converted = coded_sp_converted_norm * coded_sps_B_std + coded_sps_B_mean\n",
    "        coded_sp_converted = coded_sp_converted.T\n",
    "        coded_sp_converted = np.ascontiguousarray(coded_sp_converted)\n",
    "        decoded_sp_converted = preprocess.world_decode_spectral_envelop(\n",
    "            coded_sp=coded_sp_converted, fs=sampling_rate)\n",
    "        wav_transformed = preprocess.world_speech_synthesis(f0=f0_converted,\n",
    "                                                            decoded_sp=decoded_sp_converted,\n",
    "                                                            ap=ap,\n",
    "                                                            fs=sampling_rate,\n",
    "                                                            frame_period=frame_period)\n",
    "        librosa.output.write_wav(path=os.path.join(save_dir, os.path.basename(file)),\n",
    "                                 y=wav_transformed,\n",
    "                                 sr=sampling_rate)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_decay = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(start_epoch, num_epochs):\n",
    "    start_time_epoch = time.time()\n",
    "\n",
    "    # Constants\n",
    "    cycle_loss_lambda = 10\n",
    "    identity_loss_lambda = 5\n",
    "    #if epoch>20:#\n",
    "        #cycle_loss_lambda = 15#\n",
    "        #identity_loss_lambda = 0#\n",
    "\n",
    "    # Preparing Dataset\n",
    "    n_samples = len(dataset_A)\n",
    "\n",
    "    dataset = trainingDataset(datasetA=dataset_A,\n",
    "                              datasetB=dataset_B,\n",
    "                              n_frames=128)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                               batch_size=mini_batch_size,\n",
    "                                               shuffle=True,\n",
    "                                               drop_last=False)\n",
    "\n",
    "    for i, (real_A, real_B) in enumerate(train_loader):\n",
    "\n",
    "        num_iterations = (\n",
    "            n_samples // mini_batch_size) * epoch + i\n",
    "        # print(\"iteration no: \", num_iterations, epoch)\n",
    "\n",
    "        if num_iterations > 10000:\n",
    "            identity_loss_lambda = 0\n",
    "        if num_iterations > start_decay:\n",
    "            for generator_optimizer in generators_optimizers:\n",
    "                adjust_lr_rate(\n",
    "                    generator_optimizer, name='generator')\n",
    "                adjust_lr_rate(\n",
    "                    generator_optimizer, name='discriminator')\n",
    "        \n",
    "        real_A = real_A.to(device).float()\n",
    "        real_B = real_B.to(device).float()\n",
    "        \n",
    "        for (real1, real2, real1_is_A) in [(real_A, real_B, True), (real_B, real_A, False)]:\n",
    "            real2_is_A = not real1_is_A\n",
    "            if (real2_is_A):\n",
    "                current_realfake_discriminator = A_realfake_discriminator\n",
    "                current_realfake_discriminator_optimizer = A_realfake_discriminator_optimizer\n",
    "            else:\n",
    "                current_realfake_discriminator = B_realfake_discriminator\n",
    "                current_realfake_discriminator_optimizer = B_realfake_discriminator_optimizer\n",
    "            \n",
    "            reset_grad();\n",
    "            \n",
    "            # ----------------------------------------------------------------\n",
    "            # full forward pass and compute loss for improving generators only\n",
    "            # then backward and update generators\n",
    "            \n",
    "            # adversarial A/B loss\n",
    "            embeddings1 = encoders_forward((real1, real1, real1))\n",
    "            adversarial_AB_loss = AB_discriminator_forward_loss(embeddings1, \n",
    "                                                                isA=real1_is_A, \n",
    "                                                                gradients_for_discriminator=False)\n",
    "            \n",
    "            # identity loss\n",
    "            fakes11 = decoders_forward(embeddings1, isToA=real1_is_A)\n",
    "            identity_loss = compute_identity_loss(fakes11, real1)\n",
    "            \n",
    "            # adversarial real/fake loss\n",
    "            fakes12 = decoders_forward(embeddings1, isToA=real2_is_A)\n",
    "            \n",
    "            ## TODO: seperate A/B \n",
    "            # A_realfake_discriminator_optimizer\n",
    "            adversarial_RealFake_loss = realfake_discriminator_forward_loss(fakes12, real2, current_realfake_discriminator_optimizer,\n",
    "                                                                            current_realfake_discriminator, gradients_for_discriminator=False)\n",
    "            \n",
    "            # cycle consistency loss\n",
    "            embeddings12 = encoders_forward(fakes12)\n",
    "            fakes121 = decoders_forward(embeddings12, isToA=real1_is_A)\n",
    "            cycle_loss = compute_cycle_loss(fakes121, real1)\n",
    "            \n",
    "            # compute total generator loss\n",
    "            total_generator_loss = adversarial_AB_loss \\\n",
    "                                 + identity_loss \\\n",
    "                                 + adversarial_RealFake_loss \\\n",
    "                                 + cycle_loss\n",
    "            \n",
    "            # backward and update generators only\n",
    "            total_generator_loss.backward()\n",
    "            generators_update()\n",
    "            \n",
    "            # --------------------------------------------\n",
    "            # detach embeddings and fakes12 tensors from autograd\n",
    "            # then forward and backward the discriminators only on detached embeddings and fakes12 tensors\n",
    "            # gradients won't flow back to encoders or decoders\n",
    "            \n",
    "            AB_discriminator_zero_grad()\n",
    "            embeddings1 = (embeddings1[0].detach(), embeddings1[1].detach(), embeddings1[2].detach())\n",
    "            d_AB_loss = AB_discriminator_forward_loss(embeddings1, \n",
    "                                                      isA=real1_is_A, \n",
    "                                                      gradients_for_discriminator=True)\n",
    "            d_AB_loss.backward()\n",
    "            AB_discriminator_update()\n",
    "            \n",
    "            realfake_discriminator_zero_grad(current_realfake_discriminator_optimizer)\n",
    "            fakes12 = (fakes12[0].detach(), fakes12[1].detach(), fakes12[2].detach())    \n",
    "            d_RealFake_loss = realfake_discriminator_forward_loss(fakes12, real2, current_realfake_discriminator_optimizer,\n",
    "                                                                  current_realfake_discriminator, gradients_for_discriminator=True)\n",
    "            d_RealFake_loss.backward()\n",
    "            realfake_discriminator_update(current_realfake_discriminator_optimizer)\n",
    "            \n",
    "            total_discriminator_loss = d_RealFake_loss + d_AB_loss\n",
    "            \n",
    "            if num_iterations % 2 == 0:\n",
    "                if (real2_is_A):\n",
    "                    current_round = 'A'\n",
    "                else:\n",
    "                    current_round = 'B'\n",
    "                print(\"Iter:{}, Real Voice:{}, Generator Loss:{:.4f} || Real Fake Discrimator Loss:{:.4f} || ABDiscrimator Loss:{:.4f}\".format(\n",
    "                    num_iterations, current_round, total_generator_loss.item(), d_RealFake_loss.item(), d_AB_loss.item()))\n",
    "\n",
    "    save_dir = \"./model_complex_valid/{}/\".format(epoch)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    validation_for_A_dir(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_histogram(name, param, n_iter)\n",
    "add_histogram(\"gradient\", , global_step=None, bins='tensorflow', walltime=None, max_bins=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_noCNN."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
